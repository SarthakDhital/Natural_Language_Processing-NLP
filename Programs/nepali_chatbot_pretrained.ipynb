{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uQkA6nOkQPH"
      },
      "source": [
        "# üá≥üáµ Nepali Chatbot using Pre-trained Models\n",
        "## Building a Conversational AI with HuggingFace Transformers\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Pre-trained Models Used:\n",
        "1. **Sakonii/distilgpt2-nepali** - Nepali GPT-2 for text generation\n",
        "2. **NepBERTa/NepBERTa** - Nepali BERT for understanding\n",
        "3. **google/mt5-small** - Multilingual T5 for translation/generation\n",
        "4. **facebook/mbart-large-50** - For multilingual conversations\n",
        "\n",
        "### What We'll Build:\n",
        "- Text generation chatbot using Nepali GPT-2\n",
        "- Question-answering system\n",
        "- Retrieval-based chatbot with semantic similarity\n",
        "- Fine-tuning on custom conversation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EM_3pswkQPI"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeXXJuxfkQPJ"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "#!pip install -q transformers torch sentencepiece accelerate datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hGjMHR0kQPJ"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModel,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    pipeline,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfUg2vT7kQPJ"
      },
      "source": [
        "## 2. Load Pre-trained Nepali GPT-2 Model\n",
        "\n",
        "We'll use **Sakonii/distilgpt2-nepali** - a GPT-2 model pre-trained on 13+ million Nepali text sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES4QWskMkQPJ"
      },
      "outputs": [],
      "source": [
        "# Load Nepali GPT-2 model\n",
        "print(\"Loading Nepali GPT-2 model...\")\n",
        "\n",
        "MODEL_NAME = \"Sakonii/distilgpt2-nepali\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model = model.to(device)\n",
        "\n",
        "# Set pad token if not set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {MODEL_NAME}\")\n",
        "print(f\"   Vocabulary size: {tokenizer.vocab_size}\")\n",
        "print(f\"   Model parameters: {model.num_parameters():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBKhadLIkQPJ"
      },
      "outputs": [],
      "source": [
        "# Test basic text generation\n",
        "def generate_text(prompt, max_length=100, temperature=0.8, top_p=0.9, num_return_sequences=1):\n",
        "    \"\"\"\n",
        "    Generate Nepali text using the pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        prompt: Input text in Nepali\n",
        "        max_length: Maximum length of generated text\n",
        "        temperature: Controls randomness (lower = more focused)\n",
        "        top_p: Nucleus sampling parameter\n",
        "        num_return_sequences: Number of sequences to generate\n",
        "\n",
        "    Returns:\n",
        "        Generated text(s)\n",
        "    \"\"\"\n",
        "    # Encode input\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    generated_texts = []\n",
        "    for output in outputs:\n",
        "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        generated_texts.append(text)\n",
        "\n",
        "    return generated_texts[0] if num_return_sequences == 1 else generated_texts\n",
        "\n",
        "# Test generation\n",
        "print(\"Testing text generation...\")\n",
        "test_prompts = [\n",
        "    \"‡§®‡•á‡§™‡§æ‡§≤ ‡§è‡§ï ‡§∏‡•Å‡§®‡•ç‡§¶‡§∞\",\n",
        "    \"‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç ‡§∂‡§π‡§∞‡§Æ‡§æ\",\n",
        "    \"‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    generated = generate_text(prompt, max_length=50)\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(f\"Generated: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffjGI7eskQPJ"
      },
      "source": [
        "## 3. Create Chatbot Class using Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6goiQYPkQPJ"
      },
      "outputs": [],
      "source": [
        "class NepaliGPTChatbot:\n",
        "    \"\"\"\n",
        "    Nepali Chatbot using pre-trained GPT-2 model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.conversation_history = []\n",
        "\n",
        "        # Chat templates\n",
        "        self.user_prefix = \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: \"\n",
        "        self.bot_prefix = \"‡§¨‡•ã‡§ü: \"\n",
        "\n",
        "    def generate_response(self, user_input, max_length=100, temperature=0.7):\n",
        "        \"\"\"\n",
        "        Generate a response to user input.\n",
        "        \"\"\"\n",
        "        # Create prompt with conversation context\n",
        "        prompt = f\"{self.user_prefix}{user_input}\\n{self.bot_prefix}\"\n",
        "\n",
        "        # Add recent history for context\n",
        "        if self.conversation_history:\n",
        "            history_text = \"\\n\".join(self.conversation_history[-4:])  # Last 2 exchanges\n",
        "            prompt = history_text + \"\\n\" + prompt\n",
        "\n",
        "        # Encode\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=len(inputs[0]) + max_length,\n",
        "                temperature=temperature,\n",
        "                top_p=0.9,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                no_repeat_ngram_size=2,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        # Decode and extract response\n",
        "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract just the bot's response\n",
        "        response = full_response.split(self.bot_prefix)[-1].strip()\n",
        "\n",
        "        # Clean up - take first sentence/line\n",
        "        if \"\\n\" in response:\n",
        "            response = response.split(\"\\n\")[0]\n",
        "        if self.user_prefix in response:\n",
        "            response = response.split(self.user_prefix)[0]\n",
        "\n",
        "        # Update history\n",
        "        self.conversation_history.append(f\"{self.user_prefix}{user_input}\")\n",
        "        self.conversation_history.append(f\"{self.bot_prefix}{response}\")\n",
        "\n",
        "        return response.strip()\n",
        "\n",
        "    def chat(self, user_input):\n",
        "        \"\"\"Simple chat interface.\"\"\"\n",
        "        return self.generate_response(user_input)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset conversation history.\"\"\"\n",
        "        self.conversation_history = []\n",
        "        print(\"Conversation reset.\")\n",
        "\n",
        "# Create chatbot instance\n",
        "chatbot = NepaliGPTChatbot(model, tokenizer, device)\n",
        "print(\"‚úÖ Nepali GPT Chatbot created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iQC1bnQkQPK"
      },
      "outputs": [],
      "source": [
        "# Test the chatbot\n",
        "print(\"=\"*60)\n",
        "print(\"ü§ñ NEPALI GPT CHATBOT TEST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_inputs = [\n",
        "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\",\n",
        "    \"‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§®‡§æ‡§Æ ‡§ï‡•á ‡§π‡•ã?\",\n",
        "    \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡•á ‡§π‡•ã?\",\n",
        "    \"‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§ï‡§∏‡•ç‡§§‡•ã ‡§õ?\"\n",
        "]\n",
        "\n",
        "for user_input in test_inputs:\n",
        "    response = chatbot.chat(user_input)\n",
        "    print(f\"\\nüë§ User: {user_input}\")\n",
        "    print(f\"ü§ñ Bot: {response}\")\n",
        "\n",
        "# Reset for next test\n",
        "chatbot.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L41LtcIJkQPK"
      },
      "source": [
        "## 4. Retrieval-Based Chatbot with Semantic Similarity\n",
        "\n",
        "Using **NepBERTa** for computing semantic similarity between user query and predefined responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIe-jo4UkQPK"
      },
      "outputs": [],
      "source": [
        "# Load NepBERTa for embeddings\n",
        "print(\"Loading NepBERTa for semantic similarity...\")\n",
        "\n",
        "try:\n",
        "    nepberta_tokenizer = AutoTokenizer.from_pretrained(\"NepBERTa/NepBERTa\")\n",
        "    nepberta_model = AutoModel.from_pretrained(\"NepBERTa/NepBERTa\")\n",
        "    nepberta_model = nepberta_model.to(device)\n",
        "    print(\"‚úÖ NepBERTa loaded!\")\n",
        "    USE_NEPBERTA = True\n",
        "except Exception as e:\n",
        "    print(f\"Could not load NepBERTa: {e}\")\n",
        "    print(\"Trying multilingual BERT instead...\")\n",
        "    nepberta_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "    nepberta_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "    nepberta_model = nepberta_model.to(device)\n",
        "    print(\"‚úÖ mBERT loaded as fallback!\")\n",
        "    USE_NEPBERTA = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt0e_T4OkQPK"
      },
      "outputs": [],
      "source": [
        "# Create knowledge base for retrieval chatbot\n",
        "knowledge_base = {\n",
        "    # Greetings\n",
        "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\": \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§ï‡§∏‡§∞‡•Ä ‡§Æ‡§¶‡•ç‡§¶‡§§ ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§õ‡•Å?\",\n",
        "    \"‡§π‡•á‡§≤‡•ã\": \"‡§π‡•á‡§≤‡•ã! ‡§ï‡§∏‡•ç‡§§‡•ã ‡§π‡•Å‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ ‡§Ü‡§ú?\",\n",
        "    \"‡§ï‡•á ‡§õ\": \"‡§∏‡§¨‡•à ‡§†‡•Ä‡§ï ‡§õ, ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶! ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§ï‡•á ‡§õ?\",\n",
        "    \"‡§ï‡§∏‡•ç‡§§‡•ã ‡§õ\": \"‡§Æ ‡§†‡•Ä‡§ï ‡§õ‡•Å‡•§ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§ï‡•á‡§π‡•Ä ‡§∏‡•ã‡§ß‡•ç‡§®‡•Å ‡§õ?\",\n",
        "\n",
        "    # Identity\n",
        "    \"‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§®‡§æ‡§Æ ‡§ï‡•á ‡§π‡•ã\": \"‡§Æ ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§ö‡•ç‡§Ø‡§æ‡§ü‡§¨‡•ã‡§ü ‡§π‡•Å‡§Å, ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§∏‡§π‡§æ‡§Ø‡§ï‡•§\",\n",
        "    \"‡§§‡§™‡§æ‡§à ‡§ï‡•ã ‡§π‡•ã\": \"‡§Æ ‡§è‡§ï AI ‡§ö‡•ç‡§Ø‡§æ‡§ü‡§¨‡•ã‡§ü ‡§π‡•Å‡§Å, ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§≠‡§æ‡§∑‡§æ‡§Æ‡§æ ‡§ï‡•Å‡§∞‡§æ‡§ï‡§æ‡§®‡•Ä ‡§ó‡§∞‡•ç‡§® ‡§¨‡§®‡§æ‡§á‡§è‡§ï‡•ã‡•§\",\n",
        "    \"‡§ï‡§∏‡§≤‡•á ‡§¨‡§®‡§æ‡§Ø‡•ã ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à\": \"‡§Æ Deep Learning ‡§∞ Transformer ‡§™‡•ç‡§∞‡§µ‡§ø‡§ß‡§ø‡§¨‡§æ‡§ü ‡§¨‡§®‡§æ‡§á‡§è‡§ï‡•ã ‡§π‡•Å‡§Å‡•§\",\n",
        "\n",
        "    # Nepal Info\n",
        "    \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡•á ‡§π‡•ã\": \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç ‡§π‡•ã‡•§\",\n",
        "    \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§ú‡§®‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§ï‡§§‡§ø ‡§õ\": \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§ú‡§®‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§≤‡§ó‡§≠‡§ó ‡•© ‡§ï‡§∞‡•ã‡§° ‡§õ‡•§\",\n",
        "    \"‡§®‡•á‡§™‡§æ‡§≤ ‡§ï‡§π‡§æ‡§Å ‡§õ\": \"‡§®‡•á‡§™‡§æ‡§≤ ‡§¶‡§ï‡•ç‡§∑‡§ø‡§£ ‡§è‡§∂‡§ø‡§Ø‡§æ‡§Æ‡§æ ‡§≠‡§æ‡§∞‡§§ ‡§∞ ‡§ö‡•Ä‡§®‡§ï‡•ã ‡§¨‡•Ä‡§ö‡§Æ‡§æ ‡§Ö‡§µ‡§∏‡•ç‡§•‡§ø‡§§ ‡§õ‡•§\",\n",
        "    \"‡§∏‡§ó‡§∞‡§Æ‡§æ‡§•‡§æ‡§ï‡•ã ‡§â‡§ö‡§æ‡§à ‡§ï‡§§‡§ø ‡§π‡•ã\": \"‡§∏‡§ó‡§∞‡§Æ‡§æ‡§•‡§æ‡§ï‡•ã ‡§â‡§ö‡§æ‡§à ‡•Æ,‡•Æ‡•™‡•Æ.‡•Æ‡•¨ ‡§Æ‡§ø‡§ü‡§∞ ‡§õ‡•§\",\n",
        "    \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§´‡•Ç‡§≤ ‡§ï‡•á ‡§π‡•ã\": \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§´‡•Ç‡§≤ ‡§≤‡§æ‡§≤‡•Ä‡§ó‡•Å‡§∞‡§æ‡§Å‡§∏ ‡§π‡•ã‡•§\",\n",
        "    \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§ö‡§∞‡§æ ‡§ï‡•á ‡§π‡•ã\": \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§ö‡§∞‡§æ ‡§°‡§æ‡§Å‡§´‡•á ‡§π‡•ã‡•§\",\n",
        "\n",
        "    # Food\n",
        "    \"‡§¶‡§æ‡§≤‡§≠‡§æ‡§§ ‡§ï‡•á ‡§π‡•ã\": \"‡§¶‡§æ‡§≤‡§≠‡§æ‡§§ ‡§®‡•á‡§™‡§æ‡§≤‡•Ä‡§π‡§∞‡•Ç‡§ï‡•ã ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ñ‡§æ‡§®‡§æ ‡§π‡•ã - ‡§≠‡§æ‡§§, ‡§¶‡§æ‡§≤, ‡§§‡§∞‡§ï‡§æ‡§∞‡•Ä ‡§∞ ‡§Ö‡§ö‡§æ‡§∞‡•§\",\n",
        "    \"‡§Æ‡•ã‡§Æ‡•ã ‡§ï‡•á ‡§π‡•ã\": \"‡§Æ‡•ã‡§Æ‡•ã ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§≤‡•ã‡§ï‡§™‡•ç‡§∞‡§ø‡§Ø ‡§ñ‡§æ‡§®‡§æ ‡§π‡•ã, ‡§°‡§Æ‡•ç‡§™‡•ç‡§≤‡§ø‡§Ç‡§ó ‡§ú‡§∏‡•ç‡§§‡•ã‡•§\",\n",
        "    \"‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§ñ‡§æ‡§®‡§æ ‡§ï‡•á ‡§π‡•ã\": \"‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§ñ‡§æ‡§®‡§æ‡§Æ‡§æ ‡§¶‡§æ‡§≤‡§≠‡§æ‡§§, ‡§Æ‡•ã‡§Æ‡•ã, ‡§∏‡•á‡§≤ ‡§∞‡•ã‡§ü‡•Ä, ‡§ó‡•Å‡§®‡•ç‡§¶‡•ç‡§∞‡•Å‡§ï ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§õ‡§®‡•ç‡•§\",\n",
        "\n",
        "    # Festivals\n",
        "    \"‡§¶‡§∂‡•à‡§Ç ‡§ï‡•á ‡§π‡•ã\": \"‡§¶‡§∂‡•à‡§Ç ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∏‡§¨‡•à‡§≠‡§®‡•ç‡§¶‡§æ ‡§†‡•Ç‡§≤‡•ã ‡§ö‡§æ‡§° ‡§π‡•ã, ‡§¶‡•Å‡§∞‡•ç‡§ó‡§æ ‡§™‡•Ç‡§ú‡§æ‡§ï‡•ã ‡§∞‡•Ç‡§™‡§Æ‡§æ ‡§Æ‡§®‡§æ‡§á‡§®‡•ç‡§õ‡•§\",\n",
        "    \"‡§§‡§ø‡§π‡§æ‡§∞ ‡§ï‡•á ‡§π‡•ã\": \"‡§§‡§ø‡§π‡§æ‡§∞ ‡§¶‡•Ä‡§™‡§æ‡§µ‡§≤‡•Ä ‡§ú‡§∏‡•ç‡§§‡•ã ‡§ö‡§æ‡§° ‡§π‡•ã, ‡§™‡§æ‡§Å‡§ö ‡§¶‡§ø‡§®‡§∏‡§Æ‡•ç‡§Æ ‡§Æ‡§®‡§æ‡§á‡§®‡•ç‡§õ‡•§\",\n",
        "    \"‡§π‡•ã‡§≤‡•Ä ‡§ï‡•á ‡§π‡•ã\": \"‡§π‡•ã‡§≤‡•Ä ‡§∞‡§Ç‡§ó‡§π‡§∞‡•Ç‡§ï‡•ã ‡§ö‡§æ‡§° ‡§π‡•ã, ‡§´‡§æ‡§ó‡•Å‡§® ‡§™‡•Ç‡§∞‡•ç‡§£‡§ø‡§Æ‡§æ‡§Æ‡§æ ‡§Æ‡§®‡§æ‡§á‡§®‡•ç‡§õ‡•§\",\n",
        "\n",
        "    # Tourism\n",
        "    \"‡§™‡•ã‡§ñ‡§∞‡§æ ‡§ï‡§π‡§æ‡§Å ‡§õ\": \"‡§™‡•ã‡§ñ‡§∞‡§æ ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ‡•Ä ‡§≠‡§æ‡§ó‡§Æ‡§æ ‡§õ, ‡§´‡•á‡§µ‡§æ ‡§§‡§æ‡§≤ ‡§∞ ‡§π‡§ø‡§Æ‡§æ‡§≤‡§ï‡•ã ‡§¶‡•É‡§∂‡•ç‡§Ø‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß‡•§\",\n",
        "    \"‡§≤‡•Å‡§Æ‡•ç‡§¨‡§ø‡§®‡•Ä ‡§ï‡§ø‡§® ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§õ\": \"‡§≤‡•Å‡§Æ‡•ç‡§¨‡§ø‡§®‡•Ä ‡§≠‡§ó‡§µ‡§æ‡§® ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ï‡•ã ‡§ú‡§®‡•ç‡§Æ‡§∏‡•ç‡§•‡§æ‡§® ‡§π‡•ã, UNESCO ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§∏‡§Æ‡•ç‡§™‡§¶‡§æ ‡§∏‡•ç‡§•‡§≤‡•§\",\n",
        "    \"‡§ö‡§ø‡§§‡§µ‡§®‡§Æ‡§æ ‡§ï‡•á ‡§õ\": \"‡§ö‡§ø‡§§‡§µ‡§®‡§Æ‡§æ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§®‡§ø‡§ï‡•Å‡§û‡•ç‡§ú ‡§õ, ‡§ó‡•à‡§Ç‡§°‡§æ ‡§∞ ‡§¨‡§æ‡§ò ‡§π‡•á‡§∞‡•ç‡§® ‡§∏‡§ï‡§ø‡§®‡•ç‡§õ‡•§\",\n",
        "\n",
        "    # General\n",
        "    \"‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶\": \"‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§õ! ‡§ñ‡•Å‡§∂‡•Ä ‡§≤‡§æ‡§ó‡•ç‡§Ø‡•ã ‡§Æ‡§¶‡•ç‡§¶‡§§ ‡§ó‡§∞‡•ç‡§® ‡§™‡§æ‡§è‡§∞‡•§\",\n",
        "    \"‡§¨‡§æ‡§Ø\": \"‡§Ö‡§≤‡§µ‡§ø‡§¶‡§æ! ‡§´‡•á‡§∞‡§ø ‡§≠‡•á‡§ü‡•å‡§Ç‡§≤‡§æ!\",\n",
        "    \"‡§Æ‡§≤‡§æ‡§à ‡§Æ‡§¶‡•ç‡§¶‡§§ ‡§ö‡§æ‡§π‡§ø‡§Ø‡•ã\": \"‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§Æ‡§¶‡•ç‡§¶‡§§ ‡§ó‡§∞‡•ç‡§® ‡§§‡§Ø‡§æ‡§∞ ‡§õ‡•Å‡•§ ‡§ï‡•á ‡§ö‡§æ‡§π‡§ø‡§®‡•ç‡§õ?\",\n",
        "\n",
        "    # Weather/Time\n",
        "    \"‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§ï‡§∏‡•ç‡§§‡•ã ‡§õ\": \"‡§Æ ‡§Æ‡•å‡§∏‡§Æ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§¶‡§ø‡§® ‡§∏‡§ï‡•ç‡§¶‡§ø‡§®, ‡§§‡§∞ ‡§Ü‡§∂‡§æ ‡§ó‡§∞‡•ç‡§õ‡•Å ‡§∞‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§õ!\",\n",
        "    \"‡§ï‡§§‡§ø ‡§¨‡§ú‡•ç‡§Ø‡•ã\": \"‡§Æ ‡§∏‡§Æ‡§Ø ‡§¨‡§§‡§æ‡§â‡§® ‡§∏‡§ï‡•ç‡§¶‡§ø‡§®, ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§´‡•ã‡§® ‡§π‡•á‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç!\",\n",
        "}\n",
        "\n",
        "print(f\"Knowledge base created with {len(knowledge_base)} entries.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJr5i6Y9kQPK"
      },
      "outputs": [],
      "source": [
        "def get_embedding(text, tokenizer, model, device):\n",
        "    \"\"\"\n",
        "    Get sentence embedding using BERT model.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Use mean pooling of last hidden state\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
        "    return torch.nn.functional.cosine_similarity(a, b).item()\n",
        "\n",
        "# Pre-compute embeddings for knowledge base\n",
        "print(\"Computing embeddings for knowledge base...\")\n",
        "kb_embeddings = {}\n",
        "\n",
        "for question in knowledge_base.keys():\n",
        "    emb = get_embedding(question, nepberta_tokenizer, nepberta_model, device)\n",
        "    kb_embeddings[question] = emb\n",
        "\n",
        "print(f\"‚úÖ Computed embeddings for {len(kb_embeddings)} questions.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_URoZNlkQPL"
      },
      "outputs": [],
      "source": [
        "class NepaliRetrievalChatbot:\n",
        "    \"\"\"\n",
        "    Retrieval-based Nepali Chatbot using semantic similarity.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, knowledge_base, kb_embeddings, tokenizer, model, device, threshold=0.6):\n",
        "        self.knowledge_base = knowledge_base\n",
        "        self.kb_embeddings = kb_embeddings\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.threshold = threshold\n",
        "        self.default_response = \"‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§Æ‡•à‡§≤‡•á ‡§¨‡•Å‡§ù‡§ø‡§®‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡•á‡§∞‡§ø ‡§≠‡§®‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§\"\n",
        "\n",
        "    def find_best_match(self, user_input):\n",
        "        \"\"\"\n",
        "        Find the most similar question in knowledge base.\n",
        "        \"\"\"\n",
        "        # Get embedding for user input\n",
        "        user_emb = get_embedding(user_input, self.tokenizer, self.model, self.device)\n",
        "\n",
        "        best_match = None\n",
        "        best_score = -1\n",
        "\n",
        "        for question, emb in self.kb_embeddings.items():\n",
        "            score = cosine_similarity(user_emb, emb)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_match = question\n",
        "\n",
        "        return best_match, best_score\n",
        "\n",
        "    def chat(self, user_input):\n",
        "        \"\"\"\n",
        "        Generate response based on semantic similarity.\n",
        "        \"\"\"\n",
        "        best_match, score = self.find_best_match(user_input)\n",
        "\n",
        "        if score >= self.threshold:\n",
        "            return self.knowledge_base[best_match], best_match, score\n",
        "        else:\n",
        "            return self.default_response, None, score\n",
        "\n",
        "    def get_response(self, user_input):\n",
        "        \"\"\"Simple response without debug info.\"\"\"\n",
        "        response, _, _ = self.chat(user_input)\n",
        "        return response\n",
        "\n",
        "# Create retrieval chatbot\n",
        "retrieval_chatbot = NepaliRetrievalChatbot(\n",
        "    knowledge_base, kb_embeddings,\n",
        "    nepberta_tokenizer, nepberta_model, device,\n",
        "    threshold=0.5\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Nepali Retrieval Chatbot created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKAQ6cACkQPL"
      },
      "outputs": [],
      "source": [
        "# Test retrieval chatbot\n",
        "print(\"=\"*60)\n",
        "print(\"üîç RETRIEVAL-BASED CHATBOT TEST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_queries = [\n",
        "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\",\n",
        "    \"‡§§‡§ø‡§Æ‡•ç‡§∞‡•ã ‡§®‡§æ‡§Æ ‡§ï‡•á ‡§π‡•ã?\",\n",
        "    \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã capital ‡§ï‡•á ‡§π‡•ã?\",\n",
        "    \"‡§∏‡§ó‡§∞‡§Æ‡§æ‡§•‡§æ ‡§ï‡§§‡§ø ‡§Ö‡§ó‡•ç‡§≤‡•ã ‡§õ?\",\n",
        "    \"‡§Æ‡•ã‡§Æ‡•ã ‡§≠‡§®‡•á‡§ï‡•ã ‡§ï‡•á ‡§π‡•ã?\",\n",
        "    \"‡§¶‡§∂‡•à‡§Ç ‡§ö‡§æ‡§° ‡§ï‡§π‡§ø‡§≤‡•á ‡§π‡•ã?\",\n",
        "    \"‡§™‡•ã‡§ñ‡§∞‡§æ‡§Æ‡§æ ‡§ï‡•á ‡§π‡•á‡§∞‡•ç‡§®‡•á?\",\n",
        "    \"‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    response, matched, score = retrieval_chatbot.chat(query)\n",
        "    print(f\"\\nüë§ User: {query}\")\n",
        "    print(f\"ü§ñ Bot: {response}\")\n",
        "    if matched:\n",
        "        print(f\"   [Matched: '{matched}' | Score: {score:.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_41w_IYkQPL"
      },
      "source": [
        "## 5. Hybrid Chatbot (Retrieval + Generation)\n",
        "\n",
        "Combines both approaches for better responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1GCZwcUkQPL"
      },
      "outputs": [],
      "source": [
        "class NepaliHybridChatbot:\n",
        "    \"\"\"\n",
        "    Hybrid chatbot combining retrieval and generation.\n",
        "    Uses retrieval for known queries, generation for unknown.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, retrieval_bot, generative_bot, retrieval_threshold=0.6):\n",
        "        self.retrieval_bot = retrieval_bot\n",
        "        self.generative_bot = generative_bot\n",
        "        self.retrieval_threshold = retrieval_threshold\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def chat(self, user_input):\n",
        "        \"\"\"\n",
        "        Try retrieval first, fall back to generation.\n",
        "        \"\"\"\n",
        "        # Try retrieval first\n",
        "        response, matched, score = self.retrieval_bot.chat(user_input)\n",
        "        method = \"retrieval\"\n",
        "\n",
        "        # If retrieval confidence is low, use generation\n",
        "        if score < self.retrieval_threshold:\n",
        "            response = self.generative_bot.chat(user_input)\n",
        "            method = \"generation\"\n",
        "\n",
        "        # Store in history\n",
        "        self.conversation_history.append({\n",
        "            'user': user_input,\n",
        "            'bot': response,\n",
        "            'method': method,\n",
        "            'score': score\n",
        "        })\n",
        "\n",
        "        return response, method, score\n",
        "\n",
        "    def get_response(self, user_input):\n",
        "        \"\"\"Simple interface.\"\"\"\n",
        "        response, _, _ = self.chat(user_input)\n",
        "        return response\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset both chatbots.\"\"\"\n",
        "        self.conversation_history = []\n",
        "        self.generative_bot.reset()\n",
        "\n",
        "# Create hybrid chatbot\n",
        "hybrid_chatbot = NepaliHybridChatbot(\n",
        "    retrieval_chatbot,\n",
        "    chatbot,  # GPT-based\n",
        "    retrieval_threshold=0.55\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Nepali Hybrid Chatbot created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfZr8kSikQPL"
      },
      "outputs": [],
      "source": [
        "# Test hybrid chatbot\n",
        "print(\"=\"*60)\n",
        "print(\"üîÄ HYBRID CHATBOT TEST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_inputs = [\n",
        "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\",  # Should use retrieval\n",
        "    \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§¨‡§§‡§æ‡§â‡§®‡•Å‡§∏‡•ç\",  # Should use retrieval\n",
        "    \"‡§Ü‡§ú ‡§ï‡•á ‡§ó‡§∞‡•ç‡§®‡•á?\",  # May use generation\n",
        "    \"‡§§‡§™‡§æ‡§à‡§Ç ‡§ï‡§∏‡•ç‡§§‡•ã ‡§π‡•Å‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ?\",  # May use retrieval or generation\n",
        "    \"‡§π‡§ø‡§Æ‡§æ‡§≤ ‡§ö‡§¢‡•ç‡§® ‡§ï‡§§‡§ø ‡§ó‡§æ‡§π‡•ç‡§∞‡•ã ‡§õ?\",  # Should use generation\n",
        "    \"‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶ ‡§Æ‡§¶‡•ç‡§¶‡§§‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø\",  # Should use retrieval\n",
        "]\n",
        "\n",
        "for user_input in test_inputs:\n",
        "    response, method, score = hybrid_chatbot.chat(user_input)\n",
        "    print(f\"\\nüë§ User: {user_input}\")\n",
        "    print(f\"ü§ñ Bot: {response}\")\n",
        "    print(f\"   [Method: {method} | Score: {score:.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaXXYPPDkQPL"
      },
      "source": [
        "## 6. Fine-tune GPT-2 on Custom Nepali Conversations\n",
        "\n",
        "Let's fine-tune the pre-trained model on custom conversation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkXu7RjBkQPL"
      },
      "outputs": [],
      "source": [
        "# Create training data for fine-tuning\n",
        "conversation_data = [\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á\\n‡§¨‡•ã‡§ü: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§ï‡§∏‡§∞‡•Ä ‡§Æ‡§¶‡•ç‡§¶‡§§ ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§õ‡•Å?\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§®‡§æ‡§Æ ‡§ï‡•á ‡§π‡•ã?\\n‡§¨‡•ã‡§ü: ‡§Æ‡•á‡§∞‡•ã ‡§®‡§æ‡§Æ ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§ö‡•ç‡§Ø‡§æ‡§ü‡§¨‡•ã‡§ü ‡§π‡•ã‡•§\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡•á ‡§π‡•ã?\\n‡§¨‡•ã‡§ü: ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç ‡§π‡•ã‡•§\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§∏‡§ó‡§∞‡§Æ‡§æ‡§•‡§æ ‡§ï‡§§‡§ø ‡§Ö‡§ó‡•ç‡§≤‡•ã ‡§õ?\\n‡§¨‡•ã‡§ü: ‡§∏‡§ó‡§∞‡§Æ‡§æ‡§•‡§æ ‡•Æ,‡•Æ‡•™‡•Æ.‡•Æ‡•¨ ‡§Æ‡§ø‡§ü‡§∞ ‡§Ö‡§ó‡•ç‡§≤‡•ã ‡§õ‡•§\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§¶‡§æ‡§≤‡§≠‡§æ‡§§ ‡§ï‡•á ‡§π‡•ã?\\n‡§¨‡•ã‡§ü: ‡§¶‡§æ‡§≤‡§≠‡§æ‡§§ ‡§®‡•á‡§™‡§æ‡§≤‡•Ä‡§π‡§∞‡•Ç‡§ï‡•ã ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ñ‡§æ‡§®‡§æ ‡§π‡•ã‡•§\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§Æ‡•ã‡§Æ‡•ã ‡§ï‡§∏‡•ç‡§§‡•ã ‡§π‡•Å‡§®‡•ç‡§õ?\\n‡§¨‡•ã‡§ü: ‡§Æ‡•ã‡§Æ‡•ã ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§≤‡•ã‡§ï‡§™‡•ç‡§∞‡§ø‡§Ø ‡§ñ‡§æ‡§®‡§æ ‡§π‡•ã, ‡§°‡§Æ‡•ç‡§™‡•ç‡§≤‡§ø‡§Ç‡§ó ‡§ú‡§∏‡•ç‡§§‡•ã‡•§\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§¶‡§∂‡•à‡§Ç ‡§ï‡§π‡§ø‡§≤‡•á ‡§π‡•Å‡§®‡•ç‡§õ?\\n‡§¨‡•ã‡§ü: ‡§¶‡§∂‡•à‡§Ç ‡§Ö‡§∏‡•ã‡§ú-‡§ï‡§æ‡§∞‡•ç‡§§‡§ø‡§ï ‡§Æ‡§π‡§ø‡§®‡§æ‡§Æ‡§æ ‡§Æ‡§®‡§æ‡§á‡§®‡•ç‡§õ‡•§\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§§‡§ø‡§π‡§æ‡§∞ ‡§ï‡•á ‡§π‡•ã?\\n‡§¨‡•ã‡§ü: ‡§§‡§ø‡§π‡§æ‡§∞ ‡§¶‡•Ä‡§™‡§æ‡§µ‡§≤‡•Ä ‡§ú‡§∏‡•ç‡§§‡•ã ‡§™‡§æ‡§Å‡§ö ‡§¶‡§ø‡§®‡•á ‡§ö‡§æ‡§° ‡§π‡•ã‡•§\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§™‡•ã‡§ñ‡§∞‡§æ‡§Æ‡§æ ‡§ï‡•á ‡§õ?\\n‡§¨‡•ã‡§ü: ‡§™‡•ã‡§ñ‡§∞‡§æ‡§Æ‡§æ ‡§´‡•á‡§µ‡§æ ‡§§‡§æ‡§≤ ‡§∞ ‡§π‡§ø‡§Æ‡§æ‡§≤‡§ï‡•ã ‡§¶‡•É‡§∂‡•ç‡§Ø ‡§õ‡•§\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§≤‡•Å‡§Æ‡•ç‡§¨‡§ø‡§®‡•Ä ‡§ï‡§ø‡§® ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§õ?\\n‡§¨‡•ã‡§ü: ‡§≤‡•Å‡§Æ‡•ç‡§¨‡§ø‡§®‡•Ä ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ï‡•ã ‡§ú‡§®‡•ç‡§Æ‡§∏‡•ç‡§•‡§æ‡§® ‡§π‡•ã‡•§\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§ù‡§®‡•ç‡§°‡§æ ‡§ï‡§∏‡•ç‡§§‡•ã ‡§õ?\\n‡§¨‡•ã‡§ü: ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§ù‡§®‡•ç‡§°‡§æ ‡§¶‡•Å‡§à ‡§§‡•ç‡§∞‡§ø‡§ï‡•ã‡§£‡§ï‡•ã ‡§Ü‡§ï‡§æ‡§∞‡§Æ‡§æ ‡§õ, ‡§∏‡§Ç‡§∏‡§æ‡§∞‡§Æ‡§æ ‡§Ö‡§¶‡•ç‡§µ‡§ø‡§§‡•Ä‡§Ø‡•§\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶\\n‡§¨‡•ã‡§ü: ‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§õ! ‡§ñ‡•Å‡§∂‡•Ä ‡§≤‡§æ‡§ó‡•ç‡§Ø‡•ã ‡§Æ‡§¶‡•ç‡§¶‡§§ ‡§ó‡§∞‡•ç‡§® ‡§™‡§æ‡§è‡§∞‡•§\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§¨‡§æ‡§Ø\\n‡§¨‡•ã‡§ü: ‡§Ö‡§≤‡§µ‡§ø‡§¶‡§æ! ‡§´‡•á‡§∞‡§ø ‡§≠‡•á‡§ü‡•å‡§Ç‡§≤‡§æ!\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§ï‡§∏‡•ç‡§§‡•ã ‡§õ?\\n‡§¨‡•ã‡§ü: ‡§Æ ‡§†‡•Ä‡§ï ‡§õ‡•Å, ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶! ‡§§‡§™‡§æ‡§à‡§Ç ‡§ï‡§∏‡•ç‡§§‡•ã ‡§π‡•Å‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ?\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§Æ‡§≤‡§æ‡§à ‡§Æ‡§¶‡•ç‡§¶‡§§ ‡§ö‡§æ‡§π‡§ø‡§Ø‡•ã\\n‡§¨‡•ã‡§ü: ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§Æ‡§¶‡•ç‡§¶‡§§ ‡§ó‡§∞‡•ç‡§® ‡§§‡§Ø‡§æ‡§∞ ‡§õ‡•Å‡•§ ‡§ï‡•á ‡§ö‡§æ‡§π‡§ø‡§®‡•ç‡§õ?\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§§‡§™‡§æ‡§à ‡§ï‡•á ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ?\\n‡§¨‡•ã‡§ü: ‡§Æ ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§≠‡§æ‡§∑‡§æ‡§Æ‡§æ ‡§ï‡•Å‡§∞‡§æ‡§ï‡§æ‡§®‡•Ä ‡§∞ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§¶‡§ø‡§® ‡§∏‡§ï‡•ç‡§õ‡•Å‡•§\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§ú‡§®‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§ï‡§§‡§ø ‡§õ?\\n‡§¨‡•ã‡§ü: ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§ú‡§®‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§≤‡§ó‡§≠‡§ó ‡•© ‡§ï‡§∞‡•ã‡§° ‡§õ‡•§\",\n",
        "    \"‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§≠‡§æ‡§∑‡§æ‡§Æ‡§æ ‡§ï‡§§‡§ø ‡§Ö‡§ï‡•ç‡§∑‡§∞ ‡§õ‡§®‡•ç?\\n‡§¨‡•ã‡§ü: ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§µ‡§∞‡•ç‡§£‡§Æ‡§æ‡§≤‡§æ‡§Æ‡§æ ‡•©‡•¨ ‡§µ‡•ç‡§Ø‡§û‡•ç‡§ú‡§® ‡§∞ ‡•ß‡•® ‡§∏‡•ç‡§µ‡§∞ ‡§õ‡§®‡•ç‡•§\",\n",
        "]\n",
        "\n",
        "print(f\"Created {len(conversation_data)} training conversations.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc-3VVQakQPL"
      },
      "outputs": [],
      "source": [
        "# Prepare dataset for fine-tuning\n",
        "def tokenize_conversations(conversations, tokenizer, max_length=256):\n",
        "    \"\"\"\n",
        "    Tokenize conversation data for training.\n",
        "    \"\"\"\n",
        "    tokenized_data = []\n",
        "\n",
        "    for conv in conversations:\n",
        "        # Add EOS token at the end\n",
        "        text = conv + tokenizer.eos_token\n",
        "\n",
        "        # Tokenize\n",
        "        encodings = tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        tokenized_data.append({\n",
        "            'input_ids': encodings['input_ids'].squeeze(),\n",
        "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
        "            'labels': encodings['input_ids'].squeeze()\n",
        "        })\n",
        "\n",
        "    return tokenized_data\n",
        "\n",
        "# Tokenize\n",
        "tokenized_conversations = tokenize_conversations(conversation_data, tokenizer)\n",
        "\n",
        "# Create HuggingFace Dataset\n",
        "train_dataset = Dataset.from_list([\n",
        "    {\n",
        "        'input_ids': item['input_ids'].tolist(),\n",
        "        'attention_mask': item['attention_mask'].tolist(),\n",
        "        'labels': item['labels'].tolist()\n",
        "    }\n",
        "    for item in tokenized_conversations\n",
        "])\n",
        "\n",
        "print(f\"Dataset created with {len(train_dataset)} samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBkYIHPckQPL"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./nepali_chatbot_finetuned',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=10,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=10,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "print(\"Trainer configured!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww1ek0-YkQPL"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the model (optional - takes time)\n",
        "print(\"=\"*60)\n",
        "print(\"FINE-TUNING NEPALI GPT-2\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Uncomment to actually fine-tune\n",
        "trainer.train()\n",
        "model.save_pretrained('./nepali_chatbot_finetuned')\n",
        "tokenizer.save_pretrained('./nepali_chatbot_finetuned')\n",
        "print(\"‚úÖ Fine-tuning complete! Model saved.\")\n",
        "\n",
        "#print(\"\\n‚ö†Ô∏è Fine-tuning is commented out to save time.\")\n",
        "#print(\"Uncomment the lines above to train on your data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3977XZqkQPM"
      },
      "source": [
        "## 7. Interactive Chat Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6J9XzYfLkQPM"
      },
      "outputs": [],
      "source": [
        "def interactive_chat(chatbot_instance, chatbot_name=\"Nepali Chatbot\"):\n",
        "    \"\"\"\n",
        "    Interactive chat session.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"ü§ñ {chatbot_name} - Interactive Mode\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Type 'quit', 'exit', or '‡§¨‡§æ‡§π‡§ø‡§∞' to end the conversation.\")\n",
        "    print(\"Type 'reset' to clear conversation history.\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nüë§ You: \").strip()\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        if user_input.lower() in ['quit', 'exit', '‡§¨‡§æ‡§π‡§ø‡§∞', 'bye', '‡§¨‡§æ‡§Ø']:\n",
        "            print(\"ü§ñ Bot: ‡§Ö‡§≤‡§µ‡§ø‡§¶‡§æ! ‡§´‡•á‡§∞‡§ø ‡§≠‡•á‡§ü‡•å‡§Ç‡§≤‡§æ!\")\n",
        "            break\n",
        "\n",
        "        if user_input.lower() == 'reset':\n",
        "            if hasattr(chatbot_instance, 'reset'):\n",
        "                chatbot_instance.reset()\n",
        "            print(\"ü§ñ Bot: Conversation reset. ‡§®‡§Ø‡§æ‡§Å ‡§ï‡•Å‡§∞‡§æ‡§ï‡§æ‡§®‡•Ä ‡§∏‡•Å‡§∞‡•Å ‡§ó‡§∞‡•å‡§Ç!\")\n",
        "            continue\n",
        "\n",
        "        # Get response\n",
        "        if hasattr(chatbot_instance, 'get_response'):\n",
        "            response = chatbot_instance.get_response(user_input)\n",
        "        else:\n",
        "            response = chatbot_instance.chat(user_input)\n",
        "            if isinstance(response, tuple):\n",
        "                response = response[0]\n",
        "\n",
        "        print(f\"ü§ñ Bot: {response}\")\n",
        "\n",
        "# Uncomment to start interactive chat\n",
        "\n",
        "interactive_chat(hybrid_chatbot, \"Nepali Hybrid Chatbot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9Zi07JHBkQPM"
      },
      "outputs": [],
      "source": [
        "# Demo conversation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üá≥üáµ DEMO CONVERSATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "demo_conversation = [\n",
        "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\",\n",
        "    \"‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§®‡§æ‡§Æ ‡§ï‡•á ‡§π‡•ã?\",\n",
        "    \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§¨‡§æ‡§∞‡•á‡§Æ‡§æ ‡§ï‡•á‡§π‡•Ä ‡§¨‡§§‡§æ‡§â‡§®‡•Å‡§∏‡•ç\",\n",
        "    \"‡§∏‡§ó‡§∞‡§Æ‡§æ‡§•‡§æ ‡§ï‡§§‡§ø ‡§Ö‡§ó‡•ç‡§≤‡•ã ‡§õ?\",\n",
        "    \"‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§ñ‡§æ‡§®‡§æ ‡§ï‡•á ‡§∞‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§õ?\",\n",
        "    \"‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶, ‡§¨‡§æ‡§Ø!\"\n",
        "]\n",
        "\n",
        "hybrid_chatbot.reset()\n",
        "\n",
        "for user_input in demo_conversation:\n",
        "    response = hybrid_chatbot.get_response(user_input)\n",
        "    print(f\"\\nüë§ User: {user_input}\")\n",
        "    print(f\"ü§ñ Bot: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYsfsFoNkQPM"
      },
      "source": [
        "## 8. Save the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W-MDR2nkQPM"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import json\n",
        "\n",
        "# Save knowledge base\n",
        "with open('nepali_knowledge_base.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(knowledge_base, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úÖ Knowledge base saved to nepali_knowledge_base.json\")\n",
        "\n",
        "# Save conversation data\n",
        "with open('nepali_conversations.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(conversation_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úÖ Conversation data saved to nepali_conversations.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BJy4kNEkQPM"
      },
      "source": [
        "## 9. Summary\n",
        "\n",
        "### Pre-trained Models Used:\n",
        "\n",
        "| Model | Purpose | Source |\n",
        "|-------|---------|--------|\n",
        "| **Sakonii/distilgpt2-nepali** | Text generation | HuggingFace |\n",
        "| **NepBERTa/NepBERTa** | Semantic embeddings | HuggingFace |\n",
        "| **bert-base-multilingual-cased** | Fallback embeddings | HuggingFace |\n",
        "\n",
        "### Chatbot Types Created:\n",
        "\n",
        "| Type | Approach | Best For |\n",
        "|------|----------|----------|\n",
        "| **Generative** | GPT-2 text generation | Open-ended conversations |\n",
        "| **Retrieval** | Semantic similarity | Known Q&A pairs |\n",
        "| **Hybrid** | Both combined | General use |\n",
        "\n",
        "### Key Features:\n",
        "- Uses **actual pre-trained Nepali models**\n",
        "- Semantic similarity with NepBERTa/mBERT\n",
        "- Conversation history tracking\n",
        "- Fine-tuning capability\n",
        "- Knowledge base extensibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrdGSrGekQPM"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ NEPALI CHATBOT NOTEBOOK COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nYou have learned:\")\n",
        "print(\"  ‚úì Loading pre-trained Nepali GPT-2 model\")\n",
        "print(\"  ‚úì Building generative chatbot\")\n",
        "print(\"  ‚úì Building retrieval-based chatbot with semantic similarity\")\n",
        "print(\"  ‚úì Creating hybrid chatbot\")\n",
        "print(\"  ‚úì Fine-tuning on custom data\")\n",
        "print(\"\\nüá≥üáµ ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶! (Thank you!)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}