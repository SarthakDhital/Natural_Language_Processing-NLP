{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0LOkGJrrPw0"
      },
      "source": [
        "# Bag of Words (BoW) vs TF-IDF: Text Feature Extraction\n",
        "## A Comparative Analysis with English and Nepali Datasets\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Creating custom datasets in English and Nepali\n",
        "2. Implementing Bag of Words (BoW) feature extraction\n",
        "3. Implementing TF-IDF feature extraction\n",
        "4. Comparing both methods and understanding their differences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQIMGdHHrPw1"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuaEAtw-rPw1"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For better display\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBcrak06rPw2"
      },
      "source": [
        "## 2. Creating Custom Datasets\n",
        "\n",
        "### 2.1 English Dataset\n",
        "A collection of sentences about technology and nature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqRDbcBbrPw2"
      },
      "outputs": [],
      "source": [
        "# English Dataset - Technology and Nature themed\n",
        "english_documents = [\n",
        "    \"Machine learning is a subset of artificial intelligence\",\n",
        "    \"Deep learning uses neural networks for complex tasks\",\n",
        "    \"Natural language processing helps computers understand human language\",\n",
        "    \"The forest is home to many wild animals and birds\",\n",
        "    \"Rivers flow through valleys and provide water to villages\",\n",
        "    \"Mountains are covered with snow during winter season\",\n",
        "    \"Artificial intelligence is transforming the technology industry\",\n",
        "    \"Data science combines statistics and machine learning techniques\",\n",
        "    \"The beautiful garden has many colorful flowers and plants\",\n",
        "    \"Climate change affects forests and wildlife across the world\"\n",
        "]\n",
        "\n",
        "# Display the dataset\n",
        "print(\"English Dataset:\")\n",
        "print(\"=\"*60)\n",
        "for i, doc in enumerate(english_documents, 1):\n",
        "    print(f\"Doc {i}: {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGYQZyRtrPw2"
      },
      "source": [
        "### 2.2 Nepali Dataset\n",
        "A collection of sentences about education and daily life."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqPkOIllrPw2"
      },
      "outputs": [],
      "source": [
        "# Nepali Dataset - Education and Daily Life themed\n",
        "nepali_documents = [\n",
        "    \"à¤¨à¥‡à¤ªà¤¾à¤² à¤à¤• à¤¸à¥à¤¨à¥à¤¦à¤° à¤¦à¥‡à¤¶ à¤¹à¥‹\",\n",
        "    \"à¤•à¤¾à¤ à¤®à¤¾à¤¡à¥Œà¤‚ à¤¨à¥‡à¤ªà¤¾à¤²à¤•à¥‹ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤¹à¥‹\",\n",
        "    \"à¤µà¤¿à¤¦à¥à¤¯à¤¾à¤°à¥à¤¥à¥€à¤¹à¤°à¥‚ à¤µà¤¿à¤¦à¥à¤¯à¤¾à¤²à¤¯à¤®à¤¾ à¤ªà¤¢à¥à¤›à¤¨à¥\",\n",
        "    \"à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤œà¥€à¤µà¤¨à¤•à¥‹ à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤­à¤¾à¤— à¤¹à¥‹\",\n",
        "    \"à¤¹à¤¿à¤®à¤¾à¤²à¤¯ à¤¨à¥‡à¤ªà¤¾à¤²à¤•à¥‹ à¤—à¥Œà¤°à¤µ à¤¹à¥‹\",\n",
        "    \"à¤¨à¥‡à¤ªà¤¾à¤²à¥€ à¤­à¤¾à¤·à¤¾ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¤¿à¤¯ à¤­à¤¾à¤·à¤¾ à¤¹à¥‹\",\n",
        "    \"à¤•à¤®à¥à¤ªà¥à¤¯à¥à¤Ÿà¤° à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤†à¤œà¤•à¤² à¤§à¥‡à¤°à¥ˆ à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤›\",\n",
        "    \"à¤µà¤¿à¤¦à¥à¤¯à¤¾à¤°à¥à¤¥à¥€à¤¹à¤°à¥‚à¤²à¥‡ à¤®à¥‡à¤¹à¤¨à¤¤ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›\",\n",
        "    \"à¤ªà¥à¤°à¤µà¤¿à¤§à¤¿ à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤•à¥à¤·à¥‡à¤¤à¥à¤°à¤®à¤¾ à¤ªà¤°à¤¿à¤µà¤°à¥à¤¤à¤¨ à¤²à¥à¤¯à¤¾à¤‰à¤à¤›\",\n",
        "    \"à¤¨à¥‡à¤ªà¤¾à¤²à¤•à¥‹ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿ à¤§à¥‡à¤°à¥ˆ à¤¸à¤®à¥ƒà¤¦à¥à¤§ à¤›\"\n",
        "]\n",
        "\n",
        "# Display the dataset\n",
        "print(\"Nepali Dataset:\")\n",
        "print(\"=\"*60)\n",
        "for i, doc in enumerate(nepali_documents, 1):\n",
        "    print(f\"Doc {i}: {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXQap9pCrPw2"
      },
      "source": [
        "## 3. Bag of Words (BoW) Implementation\n",
        "\n",
        "### What is Bag of Words?\n",
        "BoW represents text as a collection of words, disregarding grammar and word order. Each document becomes a vector of word counts.\n",
        "\n",
        "**Formula:** For each word w in vocabulary V:\n",
        "```\n",
        "BoW(w, d) = count of word w in document d\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BGwBNb4rPw2"
      },
      "source": [
        "### 3.1 BoW for English Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ94_4TgrPw2"
      },
      "outputs": [],
      "source": [
        "# Initialize CountVectorizer for BoW\n",
        "bow_vectorizer_en = CountVectorizer()\n",
        "\n",
        "# Fit and transform the English documents\n",
        "bow_matrix_en = bow_vectorizer_en.fit_transform(english_documents)\n",
        "\n",
        "# Get feature names (vocabulary)\n",
        "bow_features_en = bow_vectorizer_en.get_feature_names_out()\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "bow_df_en = pd.DataFrame(\n",
        "    bow_matrix_en.toarray(),\n",
        "    columns=bow_features_en,\n",
        "    index=[f'Doc_{i+1}' for i in range(len(english_documents))]\n",
        ")\n",
        "\n",
        "print(\"Bag of Words Matrix (English):\")\n",
        "print(f\"Shape: {bow_matrix_en.shape}\")\n",
        "print(f\"Vocabulary Size: {len(bow_features_en)}\")\n",
        "print(\"\\nVocabulary:\", list(bow_features_en))\n",
        "print(\"\\nBoW Matrix:\")\n",
        "bow_df_en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_gqZS67rPw3"
      },
      "source": [
        "### 3.2 BoW for Nepali Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW-ocC07rPw3"
      },
      "outputs": [],
      "source": [
        "# For Nepali, we need to handle the script properly\n",
        "# Using a simple whitespace tokenizer since default may not work well with Devanagari\n",
        "bow_vectorizer_np = CountVectorizer(token_pattern=r'[\\u0900-\\u097F]+')\n",
        "\n",
        "# Fit and transform the Nepali documents\n",
        "bow_matrix_np = bow_vectorizer_np.fit_transform(nepali_documents)\n",
        "\n",
        "# Get feature names\n",
        "bow_features_np = bow_vectorizer_np.get_feature_names_out()\n",
        "\n",
        "# Convert to DataFrame\n",
        "bow_df_np = pd.DataFrame(\n",
        "    bow_matrix_np.toarray(),\n",
        "    columns=bow_features_np,\n",
        "    index=[f'Doc_{i+1}' for i in range(len(nepali_documents))]\n",
        ")\n",
        "\n",
        "print(\"Bag of Words Matrix (Nepali):\")\n",
        "print(f\"Shape: {bow_matrix_np.shape}\")\n",
        "print(f\"Vocabulary Size: {len(bow_features_np)}\")\n",
        "print(\"\\nVocabulary:\", list(bow_features_np))\n",
        "print(\"\\nBoW Matrix:\")\n",
        "bow_df_np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfoJ6vwerPw3"
      },
      "source": [
        "## 4. TF-IDF Implementation\n",
        "\n",
        "### What is TF-IDF?\n",
        "TF-IDF (Term Frequency - Inverse Document Frequency) weighs terms by their importance across documents.\n",
        "\n",
        "**Formulas:**\n",
        "```\n",
        "TF(t, d) = (Number of times term t appears in document d) / (Total terms in document d)\n",
        "\n",
        "IDF(t) = log(Total documents / Documents containing term t)\n",
        "\n",
        "TF-IDF(t, d) = TF(t, d) Ã— IDF(t)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe4lBXb6rPw3"
      },
      "source": [
        "### 4.1 TF-IDF for English Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqQqAGFOrPw3"
      },
      "outputs": [],
      "source": [
        "# Initialize TfidfVectorizer\n",
        "tfidf_vectorizer_en = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the English documents\n",
        "tfidf_matrix_en = tfidf_vectorizer_en.fit_transform(english_documents)\n",
        "\n",
        "# Get feature names\n",
        "tfidf_features_en = tfidf_vectorizer_en.get_feature_names_out()\n",
        "\n",
        "# Convert to DataFrame\n",
        "tfidf_df_en = pd.DataFrame(\n",
        "    tfidf_matrix_en.toarray().round(4),\n",
        "    columns=tfidf_features_en,\n",
        "    index=[f'Doc_{i+1}' for i in range(len(english_documents))]\n",
        ")\n",
        "\n",
        "print(\"TF-IDF Matrix (English):\")\n",
        "print(f\"Shape: {tfidf_matrix_en.shape}\")\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "tfidf_df_en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVdaIXsJrPw3"
      },
      "source": [
        "### 4.2 TF-IDF for Nepali Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCUSc4sOrPw3"
      },
      "outputs": [],
      "source": [
        "# TF-IDF for Nepali\n",
        "tfidf_vectorizer_np = TfidfVectorizer(token_pattern=r'[\\u0900-\\u097F]+')\n",
        "\n",
        "# Fit and transform\n",
        "tfidf_matrix_np = tfidf_vectorizer_np.fit_transform(nepali_documents)\n",
        "\n",
        "# Get feature names\n",
        "tfidf_features_np = tfidf_vectorizer_np.get_feature_names_out()\n",
        "\n",
        "# Convert to DataFrame\n",
        "tfidf_df_np = pd.DataFrame(\n",
        "    tfidf_matrix_np.toarray().round(4),\n",
        "    columns=tfidf_features_np,\n",
        "    index=[f'Doc_{i+1}' for i in range(len(nepali_documents))]\n",
        ")\n",
        "\n",
        "print(\"TF-IDF Matrix (Nepali):\")\n",
        "print(f\"Shape: {tfidf_matrix_np.shape}\")\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "tfidf_df_np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgLj6CuerPw3"
      },
      "source": [
        "## 5. Manual Calculation Example\n",
        "\n",
        "Let's manually calculate TF-IDF to understand the process better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yd-CMwJrPw3"
      },
      "outputs": [],
      "source": [
        "def manual_tfidf_calculation(documents, word, doc_index):\n",
        "    \"\"\"\n",
        "    Manually calculate TF-IDF for a specific word in a specific document.\n",
        "    \"\"\"\n",
        "    # Tokenize all documents\n",
        "    tokenized_docs = [doc.lower().split() for doc in documents]\n",
        "\n",
        "    # Calculate TF (Term Frequency)\n",
        "    target_doc = tokenized_docs[doc_index]\n",
        "    word_count = target_doc.count(word.lower())\n",
        "    tf = word_count / len(target_doc)\n",
        "\n",
        "    # Calculate IDF (Inverse Document Frequency)\n",
        "    total_docs = len(documents)\n",
        "    docs_with_word = sum(1 for doc in tokenized_docs if word.lower() in doc)\n",
        "\n",
        "    # Using log base e (natural log) + 1 smoothing\n",
        "    idf = np.log((total_docs + 1) / (docs_with_word + 1)) + 1\n",
        "\n",
        "    # Calculate TF-IDF\n",
        "    tfidf = tf * idf\n",
        "\n",
        "    return {\n",
        "        'word': word,\n",
        "        'document': doc_index + 1,\n",
        "        'word_count': word_count,\n",
        "        'doc_length': len(target_doc),\n",
        "        'TF': round(tf, 4),\n",
        "        'total_docs': total_docs,\n",
        "        'docs_with_word': docs_with_word,\n",
        "        'IDF': round(idf, 4),\n",
        "        'TF-IDF': round(tfidf, 4)\n",
        "    }\n",
        "\n",
        "# Example calculation for \"learning\" in Document 1\n",
        "print(\"Manual TF-IDF Calculation Example:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "result = manual_tfidf_calculation(english_documents, \"learning\", 0)\n",
        "print(f\"\\nWord: '{result['word']}' in Document {result['document']}\")\n",
        "print(f\"\\nStep 1: Term Frequency (TF)\")\n",
        "print(f\"  Word count in doc: {result['word_count']}\")\n",
        "print(f\"  Document length: {result['doc_length']}\")\n",
        "print(f\"  TF = {result['word_count']}/{result['doc_length']} = {result['TF']}\")\n",
        "\n",
        "print(f\"\\nStep 2: Inverse Document Frequency (IDF)\")\n",
        "print(f\"  Total documents: {result['total_docs']}\")\n",
        "print(f\"  Documents containing '{result['word']}': {result['docs_with_word']}\")\n",
        "print(f\"  IDF = log(({result['total_docs']}+1)/({result['docs_with_word']}+1)) + 1 = {result['IDF']}\")\n",
        "\n",
        "print(f\"\\nStep 3: TF-IDF\")\n",
        "print(f\"  TF-IDF = TF Ã— IDF = {result['TF']} Ã— {result['IDF']} = {result['TF-IDF']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJXtrBSvrPw3"
      },
      "source": [
        "## 6. Comparison: BoW vs TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EonKnRHdrPw3"
      },
      "outputs": [],
      "source": [
        "# Let's compare the same document using both methods\n",
        "doc_index = 0  # First document\n",
        "\n",
        "print(f\"Document: '{english_documents[doc_index]}'\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Get non-zero features for this document\n",
        "bow_values = bow_df_en.iloc[doc_index]\n",
        "tfidf_values = tfidf_df_en.iloc[doc_index]\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Word': bow_features_en,\n",
        "    'BoW (Count)': bow_values.values,\n",
        "    'TF-IDF (Weight)': tfidf_values.values.round(4)\n",
        "})\n",
        "\n",
        "# Show only words present in this document\n",
        "comparison_df = comparison_df[comparison_df['BoW (Count)'] > 0]\n",
        "comparison_df = comparison_df.sort_values('TF-IDF (Weight)', ascending=False)\n",
        "\n",
        "print(\"\\nComparison of BoW vs TF-IDF for Document 1:\")\n",
        "print(comparison_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHZRGsKOrPw3"
      },
      "source": [
        "## 7. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2OTwTfqrPw3"
      },
      "outputs": [],
      "source": [
        "# Visualize word frequencies using BoW\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Sum up word counts across all documents\n",
        "word_counts_en = bow_df_en.sum().sort_values(ascending=False)[:15]\n",
        "\n",
        "# Plot BoW frequencies\n",
        "axes[0].barh(word_counts_en.index, word_counts_en.values, color='steelblue')\n",
        "axes[0].set_xlabel('Count')\n",
        "axes[0].set_title('Top 15 Words by BoW Count (English)')\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "# Sum up TF-IDF scores\n",
        "tfidf_scores_en = tfidf_df_en.sum().sort_values(ascending=False)[:15]\n",
        "\n",
        "# Plot TF-IDF scores\n",
        "axes[1].barh(tfidf_scores_en.index, tfidf_scores_en.values, color='coral')\n",
        "axes[1].set_xlabel('TF-IDF Score')\n",
        "axes[1].set_title('Top 15 Words by TF-IDF Score (English)')\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6h3mjqMrPw3"
      },
      "outputs": [],
      "source": [
        "# Heatmap comparison for a subset of features\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Select top features for visualization\n",
        "top_features = bow_df_en.sum().sort_values(ascending=False)[:12].index\n",
        "\n",
        "# BoW Heatmap\n",
        "sns.heatmap(bow_df_en[top_features], annot=True, cmap='Blues', ax=axes[0], fmt='g')\n",
        "axes[0].set_title('Bag of Words Heatmap (English)')\n",
        "axes[0].set_xlabel('Words')\n",
        "axes[0].set_ylabel('Documents')\n",
        "\n",
        "# TF-IDF Heatmap\n",
        "sns.heatmap(tfidf_df_en[top_features], annot=True, cmap='Oranges', ax=axes[1], fmt='.2f')\n",
        "axes[1].set_title('TF-IDF Heatmap (English)')\n",
        "axes[1].set_xlabel('Words')\n",
        "axes[1].set_ylabel('Documents')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pZvWNcwrPw3"
      },
      "source": [
        "## 8. Key Differences: BoW vs TF-IDF\n",
        "\n",
        "Let's create a comprehensive comparison table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTOSOFzQrPw4"
      },
      "outputs": [],
      "source": [
        "# Create a detailed comparison table\n",
        "comparison_data = {\n",
        "    'Aspect': [\n",
        "        'Basic Concept',\n",
        "        'Value Range',\n",
        "        'Word Importance',\n",
        "        'Common Words',\n",
        "        'Rare Words',\n",
        "        'Document Length',\n",
        "        'Computation',\n",
        "        'Sparsity',\n",
        "        'Use Case',\n",
        "        'Limitation'\n",
        "    ],\n",
        "    'Bag of Words (BoW)': [\n",
        "        'Counts word occurrences',\n",
        "        'Integer counts (0, 1, 2, ...)',\n",
        "        'All words treated equally',\n",
        "        'High counts (may dominate)',\n",
        "        'Low counts',\n",
        "        'Longer docs have higher counts',\n",
        "        'Simple counting',\n",
        "        'High (many zeros)',\n",
        "        'Text classification, simple models',\n",
        "        'Ignores word importance'\n",
        "    ],\n",
        "    'TF-IDF': [\n",
        "        'Weighs words by importance',\n",
        "        'Continuous (0.0 to ~1.0)',\n",
        "        'Weighted by uniqueness',\n",
        "        'Lower weights (penalized)',\n",
        "        'Higher weights (rewarded)',\n",
        "        'Normalized by frequency',\n",
        "        'TF Ã— IDF calculation',\n",
        "        'High (many zeros)',\n",
        "        'Information retrieval, search',\n",
        "        'Ignores word order/context'\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_table = pd.DataFrame(comparison_data)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE COMPARISON: Bag of Words vs TF-IDF\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "print(comparison_table.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E9r1O8GrPw4"
      },
      "source": [
        "## 9. Practical Example: Document Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC5aa_P_rPw4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calculate cosine similarity using BoW\n",
        "bow_similarity = cosine_similarity(bow_matrix_en)\n",
        "\n",
        "# Calculate cosine similarity using TF-IDF\n",
        "tfidf_similarity = cosine_similarity(tfidf_matrix_en)\n",
        "\n",
        "# Create DataFrames for visualization\n",
        "doc_labels = [f'D{i+1}' for i in range(len(english_documents))]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# BoW Similarity Heatmap\n",
        "sns.heatmap(bow_similarity, annot=True, fmt='.2f', cmap='YlGnBu',\n",
        "            xticklabels=doc_labels, yticklabels=doc_labels, ax=axes[0])\n",
        "axes[0].set_title('Document Similarity (BoW)')\n",
        "\n",
        "# TF-IDF Similarity Heatmap\n",
        "sns.heatmap(tfidf_similarity, annot=True, fmt='.2f', cmap='YlOrRd',\n",
        "            xticklabels=doc_labels, yticklabels=doc_labels, ax=axes[1])\n",
        "axes[1].set_title('Document Similarity (TF-IDF)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDocument Reference:\")\n",
        "for i, doc in enumerate(english_documents):\n",
        "    print(f\"D{i+1}: {doc[:50]}...\" if len(doc) > 50 else f\"D{i+1}: {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtSIqg5NrPw4"
      },
      "source": [
        "## 10. Advanced Options: N-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sPVM3verPw4"
      },
      "outputs": [],
      "source": [
        "# Using bigrams (2-word combinations)\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "bigram_matrix = bigram_vectorizer.fit_transform(english_documents)\n",
        "\n",
        "print(\"Bigram (2-word) Features:\")\n",
        "print(f\"Number of bigram features: {len(bigram_vectorizer.get_feature_names_out())}\")\n",
        "print(\"\\nSample bigrams:\")\n",
        "print(list(bigram_vectorizer.get_feature_names_out())[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt8JxYnqrPw4"
      },
      "outputs": [],
      "source": [
        "# Combined unigrams and bigrams\n",
        "combined_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "combined_matrix = combined_vectorizer.fit_transform(english_documents)\n",
        "\n",
        "print(\"Combined Unigram + Bigram TF-IDF:\")\n",
        "print(f\"Total features: {len(combined_vectorizer.get_feature_names_out())}\")\n",
        "print(f\"Matrix shape: {combined_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIHMXfd0rPw4"
      },
      "source": [
        "## 11. Summary and Conclusions\n",
        "\n",
        "### When to Use BoW:\n",
        "- Simple text classification tasks\n",
        "- When word frequency matters more than importance\n",
        "- Quick baseline models\n",
        "- Small datasets\n",
        "\n",
        "### When to Use TF-IDF:\n",
        "- Information retrieval and search engines\n",
        "- Document similarity comparison\n",
        "- When distinguishing important words matters\n",
        "- Filtering out common/stop words naturally\n",
        "\n",
        "### Key Takeaways:\n",
        "1. **BoW** gives raw counts - simple but treats all words equally\n",
        "2. **TF-IDF** weighs words by importance - better for distinguishing documents\n",
        "3. Both methods lose word order and context\n",
        "4. Modern NLP often uses embeddings (Word2Vec, BERT) for better representations\n",
        "5. For Nepali text, proper tokenization is crucial due to script differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzm-KsZfrPw4"
      },
      "outputs": [],
      "source": [
        "# Final summary statistics\n",
        "print(\"=\"*60)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nðŸ“Š English Dataset:\")\n",
        "print(f\"   - Documents: {len(english_documents)}\")\n",
        "print(f\"   - Vocabulary Size: {len(bow_features_en)}\")\n",
        "print(f\"   - BoW Matrix Shape: {bow_matrix_en.shape}\")\n",
        "print(f\"   - TF-IDF Matrix Shape: {tfidf_matrix_en.shape}\")\n",
        "print(f\"   - Sparsity: {(1 - bow_matrix_en.nnz / (bow_matrix_en.shape[0] * bow_matrix_en.shape[1])) * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nðŸ“Š Nepali Dataset:\")\n",
        "print(f\"   - Documents: {len(nepali_documents)}\")\n",
        "print(f\"   - Vocabulary Size: {len(bow_features_np)}\")\n",
        "print(f\"   - BoW Matrix Shape: {bow_matrix_np.shape}\")\n",
        "print(f\"   - TF-IDF Matrix Shape: {tfidf_matrix_np.shape}\")\n",
        "print(f\"   - Sparsity: {(1 - bow_matrix_np.nnz / (bow_matrix_np.shape[0] * bow_matrix_np.shape[1])) * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nâœ… Notebook completed successfully!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}