{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification: Naive Bayes vs LSTM Comparison\n",
    "\n",
    "**Course:** BUS 405 - Foundations of Big Data Analytics  \n",
    "**Topic:** Comparative Analysis of Traditional ML and Deep Learning for Text Classification\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "1. Implement text classification using **Multinomial Naive Bayes** (traditional ML approach)\n",
    "2. Implement text classification using **LSTM** (deep learning approach)\n",
    "3. Compare performance metrics, training time, and model characteristics\n",
    "4. Understand when to use each approach\n",
    "\n",
    "## Dataset\n",
    "We'll use the **SMS Spam Collection Dataset** from Kaggle - a real-world dataset containing 5,574 SMS messages labeled as 'spam' or 'ham' (legitimate).\n",
    "\n",
    "Dataset source: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas numpy scikit-learn tensorflow matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports for Naive Bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report,\n",
    "                             roc_curve, auc)\n",
    "\n",
    "# TensorFlow/Keras imports for LSTM\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the SMS Spam Collection dataset\n",
    "# Method 1: Direct download from UCI ML Repository\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Download dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "filename = \"smsspamcollection.zip\"\n",
    "\n",
    "if not os.path.exists('SMSSpamCollection'):\n",
    "    print(\"Downloading SMS Spam Collection dataset...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    \n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "    print(\"Dataset downloaded and extracted!\")\n",
    "else:\n",
    "    print(\"Dataset already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'])\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total messages: {len(df)}\")\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nLabel Percentage:\")\n",
    "print(df['label'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar plot\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "df['label'].value_counts().plot(kind='bar', ax=axes[0], color=colors)\n",
    "axes[0].set_title('Class Distribution (Count)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Label')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "df['label'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
    "                                 colors=colors, explode=(0, 0.1))\n",
    "axes[1].set_title('Class Distribution (Percentage)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observation: The dataset is imbalanced with ~87% ham and ~13% spam messages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze message length\n",
    "df['message_length'] = df['message'].apply(len)\n",
    "df['word_count'] = df['message'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Message Length Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(df.groupby('label')[['message_length', 'word_count']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize message length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Message length distribution\n",
    "for label, color in zip(['ham', 'spam'], colors):\n",
    "    subset = df[df['label'] == label]\n",
    "    axes[0].hist(subset['message_length'], bins=50, alpha=0.7, label=label, color=color)\n",
    "axes[0].set_title('Distribution of Message Length (Characters)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Message Length')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "# Word count distribution\n",
    "for label, color in zip(['ham', 'spam'], colors):\n",
    "    subset = df[df['label'] == label]\n",
    "    axes[1].hist(subset['word_count'], bins=30, alpha=0.7, label=label, color=color)\n",
    "axes[1].set_title('Distribution of Word Count', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observation: Spam messages tend to be longer than ham messages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample messages\n",
    "print(\"Sample HAM messages:\")\n",
    "print(\"-\"*50)\n",
    "for msg in df[df['label']=='ham']['message'].sample(3, random_state=42).values:\n",
    "    print(f\"‚Ä¢ {msg[:100]}...\" if len(msg) > 100 else f\"‚Ä¢ {msg}\")\n",
    "\n",
    "print(\"\\nSample SPAM messages:\")\n",
    "print(\"-\"*50)\n",
    "for msg in df[df['label']=='spam']['message'].sample(3, random_state=42).values:\n",
    "    print(f\"‚Ä¢ {msg[:100]}...\" if len(msg) > 100 else f\"‚Ä¢ {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text data:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove special characters and digits\n",
    "    3. Remove extra whitespace\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_message'] = df['message'].apply(preprocess_text)\n",
    "\n",
    "# Convert labels to binary\n",
    "df['label_encoded'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "print(\"Preprocessing completed!\")\n",
    "print(\"\\nSample before and after:\")\n",
    "print(\"-\"*50)\n",
    "for i in [0, 1, 2]:\n",
    "    print(f\"Original: {df['message'].iloc[i][:60]}...\")\n",
    "    print(f\"Cleaned:  {df['cleaned_message'].iloc[i][:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = df['cleaned_message']\n",
    "y = df['label_encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "print(f\"\\nTraining label distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTesting label distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Naive Bayes Classification\n",
    "\n",
    "### Theory\n",
    "**Multinomial Naive Bayes** is a probabilistic classifier based on Bayes' theorem with the \"naive\" assumption of conditional independence between features.\n",
    "\n",
    "$$P(y|x_1, x_2, ..., x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i|y)}{P(x_1, x_2, ..., x_n)}$$\n",
    "\n",
    "For text classification:\n",
    "- **Features**: Word frequencies (Bag of Words) or TF-IDF scores\n",
    "- **Advantages**: Fast, works well with small datasets, interpretable\n",
    "- **Limitations**: Independence assumption, doesn't capture word order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Bag of Words with Naive Bayes\n",
    "print(\"=\"*60)\n",
    "print(\"NAIVE BAYES WITH BAG OF WORDS (CountVectorizer)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create CountVectorizer (Bag of Words)\n",
    "count_vectorizer = CountVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Transform text to feature vectors\n",
    "X_train_bow = count_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = count_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Vocabulary size: {len(count_vectorizer.vocabulary_)}\")\n",
    "print(f\"Feature matrix shape (train): {X_train_bow.shape}\")\n",
    "print(f\"Feature matrix shape (test): {X_test_bow.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes model with BOW\n",
    "start_time = time.time()\n",
    "\n",
    "nb_bow_model = MultinomialNB(alpha=1.0)  # alpha is Laplace smoothing parameter\n",
    "nb_bow_model.fit(X_train_bow, y_train)\n",
    "\n",
    "nb_bow_train_time = time.time() - start_time\n",
    "print(f\"Training time: {nb_bow_train_time:.4f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "start_time = time.time()\n",
    "y_pred_nb_bow = nb_bow_model.predict(X_test_bow)\n",
    "y_pred_proba_nb_bow = nb_bow_model.predict_proba(X_test_bow)[:, 1]\n",
    "nb_bow_inference_time = time.time() - start_time\n",
    "print(f\"Inference time: {nb_bow_inference_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: TF-IDF with Naive Bayes\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NAIVE BAYES WITH TF-IDF\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Transform text to TF-IDF features\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Feature matrix shape (train): {X_train_tfidf.shape}\")\n",
    "\n",
    "# Train Naive Bayes with TF-IDF\n",
    "start_time = time.time()\n",
    "nb_tfidf_model = MultinomialNB(alpha=1.0)\n",
    "nb_tfidf_model.fit(X_train_tfidf, y_train)\n",
    "nb_tfidf_train_time = time.time() - start_time\n",
    "print(f\"Training time: {nb_tfidf_train_time:.4f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "start_time = time.time()\n",
    "y_pred_nb_tfidf = nb_tfidf_model.predict(X_test_tfidf)\n",
    "y_pred_proba_nb_tfidf = nb_tfidf_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "nb_tfidf_inference_time = time.time() - start_time\n",
    "print(f\"Inference time: {nb_tfidf_inference_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Naive Bayes models\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate and display evaluation metrics\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "# Evaluate both NB models\n",
    "nb_bow_metrics = evaluate_model(y_test, y_pred_nb_bow, \"Naive Bayes (BOW)\")\n",
    "nb_tfidf_metrics = evaluate_model(y_test, y_pred_nb_tfidf, \"Naive Bayes (TF-IDF)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for Naive Bayes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# BOW confusion matrix\n",
    "cm_bow = confusion_matrix(y_test, y_pred_nb_bow)\n",
    "sns.heatmap(cm_bow, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "axes[0].set_title('Naive Bayes (BOW)\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# TF-IDF confusion matrix\n",
    "cm_tfidf = confusion_matrix(y_test, y_pred_nb_tfidf)\n",
    "sns.heatmap(cm_tfidf, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "axes[1].set_title('Naive Bayes (TF-IDF)\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for Naive Bayes\n",
    "def get_top_features(model, vectorizer, n=10):\n",
    "    \"\"\"Get top features for spam classification\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Log probabilities for each class\n",
    "    log_prob_spam = model.feature_log_prob_[1]  # Spam class\n",
    "    log_prob_ham = model.feature_log_prob_[0]   # Ham class\n",
    "    \n",
    "    # Difference indicates importance for spam detection\n",
    "    spam_importance = log_prob_spam - log_prob_ham\n",
    "    \n",
    "    # Get top spam indicators\n",
    "    top_spam_idx = spam_importance.argsort()[-n:][::-1]\n",
    "    top_ham_idx = spam_importance.argsort()[:n]\n",
    "    \n",
    "    return ([(feature_names[i], spam_importance[i]) for i in top_spam_idx],\n",
    "            [(feature_names[i], spam_importance[i]) for i in top_ham_idx])\n",
    "\n",
    "top_spam, top_ham = get_top_features(nb_tfidf_model, tfidf_vectorizer, n=15)\n",
    "\n",
    "print(\"Top 15 Spam Indicator Words:\")\n",
    "print(\"-\"*40)\n",
    "for word, score in top_spam:\n",
    "    print(f\"  {word:15s} : {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 15 Ham Indicator Words:\")\n",
    "print(\"-\"*40)\n",
    "for word, score in top_ham:\n",
    "    print(f\"  {word:15s} : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: LSTM Classification\n",
    "\n",
    "### Theory\n",
    "**Long Short-Term Memory (LSTM)** is a type of Recurrent Neural Network (RNN) designed to capture long-range dependencies in sequences.\n",
    "\n",
    "Key components:\n",
    "- **Forget Gate**: Decides what information to discard\n",
    "- **Input Gate**: Decides what new information to store\n",
    "- **Output Gate**: Decides what to output\n",
    "\n",
    "**Advantages**: \n",
    "- Captures sequential patterns and word order\n",
    "- Handles variable-length inputs\n",
    "- Can learn complex patterns\n",
    "\n",
    "**Limitations**: \n",
    "- Requires more data and computational resources\n",
    "- Longer training time\n",
    "- Less interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "print(\"=\"*60)\n",
    "print(\"LSTM DATA PREPARATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_VOCAB_SIZE = 10000  # Maximum vocabulary size\n",
    "MAX_SEQUENCE_LENGTH = 100  # Maximum sequence length\n",
    "EMBEDDING_DIM = 100  # Embedding dimension\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to same length\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                                padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                               padding='post', truncating='post')\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train_lstm = y_train.values\n",
    "y_test_lstm = y_test.values\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Training sequences shape: {X_train_padded.shape}\")\n",
    "print(f\"Testing sequences shape: {X_test_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How tokenization works\n",
    "sample_text = X_train.iloc[0]\n",
    "sample_seq = X_train_seq[0]\n",
    "\n",
    "print(\"Tokenization Example:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print(f\"\\nToken IDs: {sample_seq[:20]}...\")\n",
    "print(f\"\\nPadded sequence (first 20): {X_train_padded[0][:20]}\")\n",
    "\n",
    "# Reverse mapping\n",
    "reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "decoded = ' '.join([reverse_word_index.get(idx, '?') for idx in sample_seq[:10]])\n",
    "print(f\"\\nDecoded (first 10 tokens): {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM Model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LSTM MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def build_lstm_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"\n",
    "    Build LSTM model for binary text classification\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Embedding layer: converts word indices to dense vectors\n",
    "        Embedding(input_dim=vocab_size, \n",
    "                  output_dim=embedding_dim, \n",
    "                  input_length=max_length,\n",
    "                  name='embedding'),\n",
    "        \n",
    "        # LSTM layer with dropout for regularization\n",
    "        LSTM(64, return_sequences=True, dropout=0.2, name='lstm_1'),\n",
    "        LSTM(32, dropout=0.2, name='lstm_2'),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(64, activation='relu', name='dense_1'),\n",
    "        Dropout(0.5, name='dropout'),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid', name='output')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "lstm_model = build_lstm_model(\n",
    "    vocab_size=MAX_VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_length=MAX_SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM Model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING LSTM MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train_padded, y_train_lstm,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lstm_train_time = time.time() - start_time\n",
    "print(f\"\\nTotal training time: {lstm_train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0].set_title('LSTM Model Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[1].set_title('LSTM Model Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM Model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LSTM MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Predictions\n",
    "start_time = time.time()\n",
    "y_pred_proba_lstm = lstm_model.predict(X_test_padded, verbose=0)\n",
    "y_pred_lstm = (y_pred_proba_lstm > 0.5).astype(int).flatten()\n",
    "lstm_inference_time = time.time() - start_time\n",
    "\n",
    "print(f\"Inference time: {lstm_inference_time:.4f} seconds\")\n",
    "\n",
    "# Calculate metrics\n",
    "lstm_metrics = evaluate_model(y_test_lstm, y_pred_lstm, \"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "cm_lstm = confusion_matrix(y_test_lstm, y_pred_lstm)\n",
    "sns.heatmap(cm_lstm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "plt.title('LSTM Model\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report (LSTM):\")\n",
    "print(classification_report(y_test_lstm, y_pred_lstm, target_names=['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison\n",
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Model': ['Naive Bayes (BOW)', 'Naive Bayes (TF-IDF)', 'LSTM'],\n",
    "    'Accuracy': [nb_bow_metrics['accuracy'], nb_tfidf_metrics['accuracy'], lstm_metrics['accuracy']],\n",
    "    'Precision': [nb_bow_metrics['precision'], nb_tfidf_metrics['precision'], lstm_metrics['precision']],\n",
    "    'Recall': [nb_bow_metrics['recall'], nb_tfidf_metrics['recall'], lstm_metrics['recall']],\n",
    "    'F1-Score': [nb_bow_metrics['f1'], nb_tfidf_metrics['f1'], lstm_metrics['f1']],\n",
    "    'Training Time (s)': [nb_bow_train_time, nb_tfidf_train_time, lstm_train_time],\n",
    "    'Inference Time (s)': [nb_bow_inference_time, nb_tfidf_inference_time, lstm_inference_time]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "# Display comparison\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "models = ['NB (BOW)', 'NB (TF-IDF)', 'LSTM']\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "# Performance metrics comparison\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "for i, (model, color) in enumerate(zip(models, colors)):\n",
    "    values = comparison_df[comparison_df['Model'].str.contains(model.split()[0])][metrics].values[0]\n",
    "    axes[0].bar(x + i*width, values, width, label=model, color=color, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Metrics')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Performance Metrics Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0.8, 1.0)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Training time comparison\n",
    "train_times = comparison_df['Training Time (s)'].values\n",
    "bars = axes[1].bar(models, train_times, color=colors, alpha=0.8)\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Time (seconds)')\n",
    "axes[1].set_title('Training Time Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, time in zip(bars, train_times):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                 f'{time:.2f}s', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Inference time comparison\n",
    "inference_times = comparison_df['Inference Time (s)'].values\n",
    "bars = axes[2].bar(models, inference_times, color=colors, alpha=0.8)\n",
    "axes[2].set_xlabel('Model')\n",
    "axes[2].set_ylabel('Time (seconds)')\n",
    "axes[2].set_title('Inference Time Comparison', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, time in zip(bars, inference_times):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                 f'{time:.4f}s', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Comparison\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate ROC curves\n",
    "fpr_bow, tpr_bow, _ = roc_curve(y_test, y_pred_proba_nb_bow)\n",
    "roc_auc_bow = auc(fpr_bow, tpr_bow)\n",
    "\n",
    "fpr_tfidf, tpr_tfidf, _ = roc_curve(y_test, y_pred_proba_nb_tfidf)\n",
    "roc_auc_tfidf = auc(fpr_tfidf, tpr_tfidf)\n",
    "\n",
    "fpr_lstm, tpr_lstm, _ = roc_curve(y_test_lstm, y_pred_proba_lstm)\n",
    "roc_auc_lstm = auc(fpr_lstm, tpr_lstm)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.plot(fpr_bow, tpr_bow, color='#3498db', lw=2, \n",
    "         label=f'Naive Bayes (BOW) - AUC = {roc_auc_bow:.4f}')\n",
    "plt.plot(fpr_tfidf, tpr_tfidf, color='#2ecc71', lw=2, \n",
    "         label=f'Naive Bayes (TF-IDF) - AUC = {roc_auc_tfidf:.4f}')\n",
    "plt.plot(fpr_lstm, tpr_lstm, color='#e74c3c', lw=2, \n",
    "         label=f'LSTM - AUC = {roc_auc_lstm:.4f}')\n",
    "\n",
    "# Diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random Classifier')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAUC Scores:\")\n",
    "print(f\"  Naive Bayes (BOW):    {roc_auc_bow:.4f}\")\n",
    "print(f\"  Naive Bayes (TF-IDF): {roc_auc_tfidf:.4f}\")\n",
    "print(f\"  LSTM:                 {roc_auc_lstm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All confusion matrices side by side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "cms = [cm_bow, cm_tfidf, cm_lstm]\n",
    "titles = ['Naive Bayes (BOW)', 'Naive Bayes (TF-IDF)', 'LSTM']\n",
    "cmaps = ['Blues', 'Blues', 'Greens']\n",
    "\n",
    "for ax, cm, title, cmap in zip(axes, cms, titles, cmaps):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, ax=ax,\n",
    "                xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "    ax.set_title(f'{title}', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "plt.suptitle('Confusion Matrix Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample messages\n",
    "sample_messages = [\n",
    "    \"Congratulations! You've won a $1000 gift card. Click here to claim now!\",\n",
    "    \"Hey, are we still meeting for lunch tomorrow at 2pm?\",\n",
    "    \"URGENT: Your account has been compromised. Call this number immediately.\",\n",
    "    \"Thanks for your help yesterday. I really appreciate it!\",\n",
    "    \"FREE entry to win iPhone 15! Text WIN to 12345 now!\",\n",
    "    \"Can you pick up some groceries on your way home?\"\n",
    "]\n",
    "\n",
    "def predict_message(message, models_dict):\n",
    "    \"\"\"\n",
    "    Predict spam/ham for a given message using all models\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    cleaned = preprocess_text(message)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Naive Bayes (TF-IDF) prediction\n",
    "    tfidf_vec = tfidf_vectorizer.transform([cleaned])\n",
    "    nb_pred = nb_tfidf_model.predict(tfidf_vec)[0]\n",
    "    nb_prob = nb_tfidf_model.predict_proba(tfidf_vec)[0][1]\n",
    "    results['NB (TF-IDF)'] = ('Spam' if nb_pred == 1 else 'Ham', nb_prob)\n",
    "    \n",
    "    # LSTM prediction\n",
    "    seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    lstm_prob = lstm_model.predict(padded, verbose=0)[0][0]\n",
    "    lstm_pred = 1 if lstm_prob > 0.5 else 0\n",
    "    results['LSTM'] = ('Spam' if lstm_pred == 1 else 'Ham', lstm_prob)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Make predictions\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, msg in enumerate(sample_messages, 1):\n",
    "    print(f\"\\nüìß Message {i}: \\\"{msg[:60]}...\\\"\" if len(msg) > 60 else f\"\\nüìß Message {i}: \\\"{msg}\\\"\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    results = predict_message(msg, None)\n",
    "    \n",
    "    for model, (prediction, prob) in results.items():\n",
    "        icon = \"üö´\" if prediction == 'Spam' else \"‚úÖ\"\n",
    "        print(f\"  {model:15s}: {icon} {prediction:4s} (confidence: {prob:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary Table\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Add AUC to comparison\n",
    "comparison_df['AUC'] = [roc_auc_bow, roc_auc_tfidf, roc_auc_lstm]\n",
    "\n",
    "# Display final comparison\n",
    "print(\"\\nüìä Performance Metrics:\")\n",
    "print(comparison_df[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']].to_string(index=False))\n",
    "\n",
    "print(\"\\n‚è±Ô∏è Computational Efficiency:\")\n",
    "print(comparison_df[['Model', 'Training Time (s)', 'Inference Time (s)']].to_string(index=False))\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\nüèÜ Best Model by Metric:\")\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']:\n",
    "    best_idx = comparison_df[metric].idxmax()\n",
    "    best_model = comparison_df.loc[best_idx, 'Model']\n",
    "    best_value = comparison_df.loc[best_idx, metric]\n",
    "    print(f\"  {metric:12s}: {best_model} ({best_value:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings and Recommendations\n",
    "\n",
    "### Performance Analysis\n",
    "\n",
    "| Aspect | Naive Bayes | LSTM |\n",
    "|--------|-------------|------|\n",
    "| **Accuracy** | High (~96-98%) | High (~97-99%) |\n",
    "| **Training Speed** | Very fast (<1 sec) | Slow (minutes) |\n",
    "| **Inference Speed** | Very fast | Slower |\n",
    "| **Interpretability** | High (feature importance) | Low (black box) |\n",
    "| **Data Requirements** | Works with small data | Needs more data |\n",
    "| **Sequential Patterns** | No | Yes |\n",
    "\n",
    "### When to Use Each Model\n",
    "\n",
    "**Choose Naive Bayes when:**\n",
    "- You have limited computational resources\n",
    "- You need quick prototyping and baseline\n",
    "- Dataset is small to medium-sized\n",
    "- Interpretability is important\n",
    "- Real-time predictions are required\n",
    "\n",
    "**Choose LSTM when:**\n",
    "- Word order and context matter significantly\n",
    "- You have large datasets\n",
    "- Computational resources are available\n",
    "- Higher accuracy is worth the extra complexity\n",
    "- You're dealing with longer text sequences\n",
    "\n",
    "### Practical Recommendations for SMS Spam Detection\n",
    "\n",
    "1. **For production systems**: Consider Naive Bayes due to its speed and competitive accuracy\n",
    "2. **For research/high-stakes**: LSTM or transformer models may provide marginal improvements\n",
    "3. **Ensemble approach**: Combine both models for robust predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises for Students\n",
    "\n",
    "1. **Experiment with hyperparameters:**\n",
    "   - Try different `max_features` in vectorizers\n",
    "   - Modify LSTM architecture (add/remove layers, change units)\n",
    "   - Adjust `alpha` parameter in Naive Bayes\n",
    "\n",
    "2. **Try different preprocessing:**\n",
    "   - Add stemming or lemmatization\n",
    "   - Remove/keep numbers\n",
    "   - Experiment with n-grams\n",
    "\n",
    "3. **Advanced models:**\n",
    "   - Implement Bidirectional LSTM\n",
    "   - Try GRU instead of LSTM\n",
    "   - Implement attention mechanism\n",
    "\n",
    "4. **Handle class imbalance:**\n",
    "   - Apply SMOTE\n",
    "   - Use class weights\n",
    "   - Try undersampling/oversampling\n",
    "\n",
    "5. **Deploy the model:**\n",
    "   - Create a simple Flask/FastAPI endpoint\n",
    "   - Build a Streamlit dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models for future use\n",
    "import pickle\n",
    "\n",
    "# Save Naive Bayes model and vectorizer\n",
    "with open('nb_tfidf_model.pkl', 'wb') as f:\n",
    "    pickle.dump({'model': nb_tfidf_model, 'vectorizer': tfidf_vectorizer}, f)\n",
    "\n",
    "# Save LSTM model\n",
    "lstm_model.save('lstm_spam_classifier.h5')\n",
    "\n",
    "# Save tokenizer\n",
    "with open('lstm_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"‚úÖ Models saved successfully!\")\n",
    "print(\"   - nb_tfidf_model.pkl\")\n",
    "print(\"   - lstm_spam_classifier.h5\")\n",
    "print(\"   - lstm_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Almeida, T.A., G√≥mez Hidalgo, J.M., Yamakami, A. (2011). Contributions to the Study of SMS Spam Filtering: New Collection and Results. DocEng'11.\n",
    "\n",
    "2. Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation.\n",
    "\n",
    "3. McCallum, A., & Nigam, K. (1998). A Comparison of Event Models for Naive Bayes Text Classification. AAAI Workshop.\n",
    "\n",
    "4. Dataset: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook prepared for BUS 405: Foundations of Big Data Analytics*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
