{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Exercise: Naive Bayes Text Classification & N-gram Language Models\n",
    "\n",
    "**Course:** BUS 405 - Foundations of Big Data Analytics  \n",
    "**Institution:** Pokhara University\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Implement Naive Bayes classifier from scratch with Laplace smoothing\n",
    "2. Understand and build N-gram language models (unigram, bigram, trigram)\n",
    "3. Combine N-gram features with Naive Bayes for improved text classification\n",
    "4. Evaluate classifier performance using various metrics\n",
    "5. Apply these techniques to real-world text classification problems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn for comparison and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Naive Bayes Classification\n",
    "\n",
    "### 2.1 Theory Review\n",
    "\n",
    "**Bayes' Theorem:**\n",
    "$$P(Class|Document) = \\frac{P(Document|Class) \\times P(Class)}{P(Document)}$$\n",
    "\n",
    "**Naive Assumption:** All features (words) are conditionally independent:\n",
    "$$P(w_1, w_2, ..., w_n|Class) = \\prod_{i=1}^{n} P(w_i|Class)$$\n",
    "\n",
    "**Laplace Smoothing:**\n",
    "$$P(w_i|Class) = \\frac{count(w_i, Class) + k}{total\\_words\\_in\\_Class + k \\times |V|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sample Dataset: Nepali Business Review Classification\n",
    "\n",
    "Let's create a sample dataset with business reviews categorized as **Positive** or **Negative**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset - Business reviews (English for simplicity)\n",
    "training_data = [\n",
    "    # Positive reviews\n",
    "    (\"excellent service great food highly recommend\", \"positive\"),\n",
    "    (\"amazing experience wonderful staff friendly service\", \"positive\"),\n",
    "    (\"best restaurant excellent quality delicious food\", \"positive\"),\n",
    "    (\"great value good prices excellent location\", \"positive\"),\n",
    "    (\"wonderful atmosphere friendly staff highly recommend\", \"positive\"),\n",
    "    (\"outstanding service great food amazing experience\", \"positive\"),\n",
    "    (\"excellent product fast delivery good quality\", \"positive\"),\n",
    "    (\"highly recommend great service best prices\", \"positive\"),\n",
    "    \n",
    "    # Negative reviews\n",
    "    (\"terrible service bad food never again\", \"negative\"),\n",
    "    (\"worst experience rude staff poor quality\", \"negative\"),\n",
    "    (\"bad service slow delivery disappointed\", \"negative\"),\n",
    "    (\"poor quality overpriced terrible experience\", \"negative\"),\n",
    "    (\"horrible food bad service waste money\", \"negative\"),\n",
    "    (\"disappointing experience poor staff bad quality\", \"negative\"),\n",
    "    (\"never recommend terrible service bad food\", \"negative\"),\n",
    "    (\"awful experience worst service poor quality\", \"negative\"),\n",
    "]\n",
    "\n",
    "# Test data\n",
    "test_data = [\n",
    "    (\"great service excellent food\", \"positive\"),\n",
    "    (\"bad experience terrible service\", \"negative\"),\n",
    "    (\"good quality friendly staff\", \"positive\"),\n",
    "    (\"poor service disappointed\", \"negative\"),\n",
    "]\n",
    "\n",
    "# Display as DataFrame\n",
    "df_train = pd.DataFrame(training_data, columns=['text', 'label'])\n",
    "print(\"Training Data:\")\n",
    "print(df_train)\n",
    "print(f\"\\nClass distribution: {df_train['label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Implementing Naive Bayes from Scratch\n",
    "\n",
    "### 3.1 Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Basic text preprocessing:\n",
    "    - Convert to lowercase\n",
    "    - Remove special characters\n",
    "    - Tokenize into words\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test preprocessing\n",
    "sample_text = \"Excellent Service! Great food... Highly Recommend!!!\"\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Preprocessed: {preprocess_text(sample_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Naive Bayes Classifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    \"\"\"\n",
    "    Multinomial Naive Bayes Classifier with Laplace Smoothing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=1):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        k : float\n",
    "            Smoothing parameter (default=1 for Laplace smoothing)\n",
    "        \"\"\"\n",
    "        self.k = k  # Smoothing parameter\n",
    "        self.class_priors = {}  # P(class)\n",
    "        self.word_counts = {}  # count(word, class)\n",
    "        self.class_word_totals = {}  # total words per class\n",
    "        self.vocabulary = set()  # unique words\n",
    "        self.classes = []\n",
    "        \n",
    "    def fit(self, documents, labels):\n",
    "        \"\"\"\n",
    "        Train the classifier on the given documents and labels.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        documents : list of str\n",
    "            Training documents\n",
    "        labels : list of str\n",
    "            Corresponding labels\n",
    "        \"\"\"\n",
    "        self.classes = list(set(labels))\n",
    "        n_docs = len(documents)\n",
    "        \n",
    "        # Initialize data structures\n",
    "        for c in self.classes:\n",
    "            self.word_counts[c] = defaultdict(int)\n",
    "            self.class_word_totals[c] = 0\n",
    "        \n",
    "        # Count documents per class and words per class\n",
    "        class_doc_counts = Counter(labels)\n",
    "        \n",
    "        for doc, label in zip(documents, labels):\n",
    "            tokens = preprocess_text(doc)\n",
    "            for word in tokens:\n",
    "                self.vocabulary.add(word)\n",
    "                self.word_counts[label][word] += 1\n",
    "                self.class_word_totals[label] += 1\n",
    "        \n",
    "        # Calculate class priors: P(class) = count(class) / total_docs\n",
    "        for c in self.classes:\n",
    "            self.class_priors[c] = class_doc_counts[c] / n_docs\n",
    "            \n",
    "        print(f\"Training completed!\")\n",
    "        print(f\"Vocabulary size: {len(self.vocabulary)}\")\n",
    "        print(f\"Class priors: {self.class_priors}\")\n",
    "        \n",
    "    def _calculate_word_probability(self, word, class_label):\n",
    "        \"\"\"\n",
    "        Calculate P(word|class) with Laplace smoothing.\n",
    "        \n",
    "        Formula: P(w|c) = (count(w,c) + k) / (total_words_c + k * |V|)\n",
    "        \"\"\"\n",
    "        word_count = self.word_counts[class_label].get(word, 0)\n",
    "        total_words = self.class_word_totals[class_label]\n",
    "        vocab_size = len(self.vocabulary)\n",
    "        \n",
    "        probability = (word_count + self.k) / (total_words + self.k * vocab_size)\n",
    "        return probability\n",
    "    \n",
    "    def predict_proba(self, document, verbose=False):\n",
    "        \"\"\"\n",
    "        Calculate probability of each class for a document.\n",
    "        \n",
    "        Returns dict of {class: probability}\n",
    "        \"\"\"\n",
    "        tokens = preprocess_text(document)\n",
    "        class_scores = {}\n",
    "        \n",
    "        for c in self.classes:\n",
    "            # Start with log of prior probability\n",
    "            log_prob = math.log(self.class_priors[c])\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nClass: {c}\")\n",
    "                print(f\"  Prior P({c}) = {self.class_priors[c]:.4f}\")\n",
    "                print(f\"  Log prior = {log_prob:.4f}\")\n",
    "            \n",
    "            # Add log probabilities of each word\n",
    "            for word in tokens:\n",
    "                word_prob = self._calculate_word_probability(word, c)\n",
    "                log_prob += math.log(word_prob)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  P({word}|{c}) = {word_prob:.4f}, log = {math.log(word_prob):.4f}\")\n",
    "            \n",
    "            class_scores[c] = log_prob\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Total log probability: {log_prob:.4f}\")\n",
    "        \n",
    "        # Convert log probabilities to probabilities (normalized)\n",
    "        max_log = max(class_scores.values())\n",
    "        exp_scores = {c: math.exp(s - max_log) for c, s in class_scores.items()}\n",
    "        total = sum(exp_scores.values())\n",
    "        probabilities = {c: s / total for c, s in exp_scores.items()}\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, document):\n",
    "        \"\"\"\n",
    "        Predict the class of a document.\n",
    "        \"\"\"\n",
    "        probabilities = self.predict_proba(document)\n",
    "        return max(probabilities, key=probabilities.get)\n",
    "    \n",
    "    def evaluate(self, documents, labels):\n",
    "        \"\"\"\n",
    "        Evaluate the classifier on test data.\n",
    "        \"\"\"\n",
    "        predictions = [self.predict(doc) for doc in documents]\n",
    "        \n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        precision = precision_score(labels, predictions, pos_label='positive')\n",
    "        recall = recall_score(labels, predictions, pos_label='positive')\n",
    "        f1 = f1_score(labels, predictions, pos_label='positive')\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'predictions': predictions\n",
    "        }\n",
    "\n",
    "print(\"NaiveBayesClassifier class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training and Testing the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "train_docs = [text for text, label in training_data]\n",
    "train_labels = [label for text, label in training_data]\n",
    "\n",
    "# Initialize and train the classifier\n",
    "nb_classifier = NaiveBayesClassifier(k=1)  # k=1 for Laplace smoothing\n",
    "nb_classifier.fit(train_docs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed prediction example with step-by-step calculation\n",
    "test_doc = \"great service excellent food\"\n",
    "print(f\"Testing document: '{test_doc}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "probabilities = nb_classifier.predict_proba(test_doc, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nFinal Probabilities:\")\n",
    "for c, prob in probabilities.items():\n",
    "    print(f\"  P({c}|document) = {prob:.4f} ({prob*100:.2f}%)\")\n",
    "\n",
    "prediction = nb_classifier.predict(test_doc)\n",
    "print(f\"\\nPrediction: {prediction.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on all test documents\n",
    "print(\"Testing on all test documents:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_docs = [text for text, label in test_data]\n",
    "test_labels = [label for text, label in test_data]\n",
    "\n",
    "for doc, true_label in test_data:\n",
    "    pred = nb_classifier.predict(doc)\n",
    "    probs = nb_classifier.predict_proba(doc)\n",
    "    status = \"âœ“\" if pred == true_label else \"âœ—\"\n",
    "    print(f\"{status} '{doc}'\")\n",
    "    print(f\"   True: {true_label}, Predicted: {pred}\")\n",
    "    print(f\"   Probabilities: pos={probs['positive']:.3f}, neg={probs['negative']:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier\n",
    "results = nb_classifier.evaluate(test_docs, test_labels)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Accuracy:  {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)\")\n",
    "print(f\"Precision: {results['precision']:.4f}\")\n",
    "print(f\"Recall:    {results['recall']:.4f}\")\n",
    "print(f\"F1-Score:  {results['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: N-gram Language Models\n",
    "\n",
    "### 4.1 Theory Review\n",
    "\n",
    "N-gram models predict the probability of a word based on the previous (n-1) words:\n",
    "\n",
    "- **Unigram:** $P(w_i)$ - no context\n",
    "- **Bigram:** $P(w_i|w_{i-1})$ - one word context\n",
    "- **Trigram:** $P(w_i|w_{i-2}, w_{i-1})$ - two words context\n",
    "\n",
    "### 4.2 N-gram Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens, n):\n",
    "    \"\"\"\n",
    "    Extract n-grams from a list of tokens.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tokens : list of str\n",
    "        List of words\n",
    "    n : int\n",
    "        N-gram size (1=unigram, 2=bigram, 3=trigram)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list of tuples\n",
    "        List of n-grams\n",
    "    \"\"\"\n",
    "    # Add start and end tokens for proper boundary handling\n",
    "    padded_tokens = ['<s>'] * (n-1) + tokens + ['</s>']\n",
    "    \n",
    "    ngrams = []\n",
    "    for i in range(len(padded_tokens) - n + 1):\n",
    "        ngram = tuple(padded_tokens[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "# Demonstration\n",
    "sample_sentence = \"I love NLP\"\n",
    "tokens = preprocess_text(sample_sentence)\n",
    "print(f\"Original: '{sample_sentence}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print()\n",
    "\n",
    "for n in [1, 2, 3]:\n",
    "    ngrams = extract_ngrams(tokens, n)\n",
    "    print(f\"{n}-grams: {ngrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 N-gram Language Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    \"\"\"\n",
    "    N-gram Language Model with Laplace Smoothing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n=2, k=1):\n",
    "        \"\"\"\n",
    "        Initialize the language model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n : int\n",
    "            N-gram order (1=unigram, 2=bigram, 3=trigram)\n",
    "        k : float\n",
    "            Smoothing parameter\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.ngram_counts = defaultdict(int)  # count of n-grams\n",
    "        self.context_counts = defaultdict(int)  # count of (n-1)-grams\n",
    "        self.vocabulary = set()\n",
    "        \n",
    "    def fit(self, documents):\n",
    "        \"\"\"\n",
    "        Train the language model on documents.\n",
    "        \"\"\"\n",
    "        for doc in documents:\n",
    "            tokens = preprocess_text(doc)\n",
    "            self.vocabulary.update(tokens)\n",
    "            \n",
    "            ngrams = extract_ngrams(tokens, self.n)\n",
    "            for ngram in ngrams:\n",
    "                self.ngram_counts[ngram] += 1\n",
    "                context = ngram[:-1]  # All but last word\n",
    "                self.context_counts[context] += 1\n",
    "        \n",
    "        # Add special tokens to vocabulary\n",
    "        self.vocabulary.add('<s>')\n",
    "        self.vocabulary.add('</s>')\n",
    "        \n",
    "        print(f\"{self.n}-gram model trained!\")\n",
    "        print(f\"Vocabulary size: {len(self.vocabulary)}\")\n",
    "        print(f\"Unique {self.n}-grams: {len(self.ngram_counts)}\")\n",
    "        \n",
    "    def probability(self, word, context=None):\n",
    "        \"\"\"\n",
    "        Calculate P(word|context) with smoothing.\n",
    "        \n",
    "        For bigram: P(w2|w1) = (count(w1,w2) + k) / (count(w1) + k*|V|)\n",
    "        \"\"\"\n",
    "        if self.n == 1:  # Unigram\n",
    "            count = self.ngram_counts.get((word,), 0)\n",
    "            total = sum(self.ngram_counts.values())\n",
    "            return (count + self.k) / (total + self.k * len(self.vocabulary))\n",
    "        else:\n",
    "            if context is None:\n",
    "                context = tuple(['<s>'] * (self.n - 1))\n",
    "            ngram = context + (word,)\n",
    "            ngram_count = self.ngram_counts.get(ngram, 0)\n",
    "            context_count = self.context_counts.get(context, 0)\n",
    "            \n",
    "            return (ngram_count + self.k) / (context_count + self.k * len(self.vocabulary))\n",
    "    \n",
    "    def sentence_probability(self, sentence, log_prob=True):\n",
    "        \"\"\"\n",
    "        Calculate the probability of an entire sentence.\n",
    "        \"\"\"\n",
    "        tokens = preprocess_text(sentence)\n",
    "        ngrams = extract_ngrams(tokens, self.n)\n",
    "        \n",
    "        total_log_prob = 0\n",
    "        probabilities = []\n",
    "        \n",
    "        for ngram in ngrams:\n",
    "            context = ngram[:-1]\n",
    "            word = ngram[-1]\n",
    "            prob = self.probability(word, context)\n",
    "            probabilities.append((ngram, prob))\n",
    "            total_log_prob += math.log(prob)\n",
    "        \n",
    "        if log_prob:\n",
    "            return total_log_prob, probabilities\n",
    "        else:\n",
    "            return math.exp(total_log_prob), probabilities\n",
    "    \n",
    "    def perplexity(self, sentence):\n",
    "        \"\"\"\n",
    "        Calculate perplexity of a sentence.\n",
    "        PPL = exp(-1/N * sum(log P(wi|context)))\n",
    "        \"\"\"\n",
    "        tokens = preprocess_text(sentence)\n",
    "        ngrams = extract_ngrams(tokens, self.n)\n",
    "        N = len(ngrams)\n",
    "        \n",
    "        log_prob, _ = self.sentence_probability(sentence)\n",
    "        perplexity = math.exp(-log_prob / N)\n",
    "        \n",
    "        return perplexity\n",
    "\n",
    "print(\"NGramLanguageModel class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Training and Comparing N-gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus for language model training\n",
    "corpus = [\n",
    "    \"I love machine learning\",\n",
    "    \"I love natural language processing\",\n",
    "    \"machine learning is amazing\",\n",
    "    \"natural language processing is great\",\n",
    "    \"I study data science\",\n",
    "    \"data science uses machine learning\",\n",
    "    \"deep learning is powerful\",\n",
    "    \"I love deep learning\",\n",
    "]\n",
    "\n",
    "print(\"Training Corpus:\")\n",
    "for i, sent in enumerate(corpus, 1):\n",
    "    print(f\"  {i}. {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train different n-gram models\n",
    "print(\"Training N-gram Models:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "unigram_model = NGramLanguageModel(n=1, k=1)\n",
    "unigram_model.fit(corpus)\n",
    "print()\n",
    "\n",
    "bigram_model = NGramLanguageModel(n=2, k=1)\n",
    "bigram_model.fit(corpus)\n",
    "print()\n",
    "\n",
    "trigram_model = NGramLanguageModel(n=3, k=1)\n",
    "trigram_model.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sentence probabilities across models\n",
    "test_sentences = [\n",
    "    \"I love machine learning\",  # Seen in training\n",
    "    \"I love data science\",       # Partially seen\n",
    "    \"machine learning is great\", # Combination of seen patterns\n",
    "    \"xyz abc def\",               # Unseen words\n",
    "]\n",
    "\n",
    "print(\"Sentence Probability Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "for sent in test_sentences:\n",
    "    uni_log, _ = unigram_model.sentence_probability(sent)\n",
    "    bi_log, _ = bigram_model.sentence_probability(sent)\n",
    "    tri_log, _ = trigram_model.sentence_probability(sent)\n",
    "    \n",
    "    uni_ppl = unigram_model.perplexity(sent)\n",
    "    bi_ppl = bigram_model.perplexity(sent)\n",
    "    tri_ppl = trigram_model.perplexity(sent)\n",
    "    \n",
    "    results.append({\n",
    "        'Sentence': sent,\n",
    "        'Unigram Log P': f\"{uni_log:.3f}\",\n",
    "        'Bigram Log P': f\"{bi_log:.3f}\",\n",
    "        'Trigram Log P': f\"{tri_log:.3f}\",\n",
    "        'Unigram PPL': f\"{uni_ppl:.2f}\",\n",
    "        'Bigram PPL': f\"{bi_ppl:.2f}\",\n",
    "        'Trigram PPL': f\"{tri_ppl:.2f}\",\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed breakdown for one sentence\n",
    "example_sentence = \"I love machine learning\"\n",
    "print(f\"\\nDetailed Breakdown for: '{example_sentence}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model, name in [(bigram_model, 'Bigram')]:\n",
    "    print(f\"\\n{name} Model:\")\n",
    "    log_prob, probs = model.sentence_probability(example_sentence)\n",
    "    \n",
    "    for ngram, prob in probs:\n",
    "        context = ngram[:-1]\n",
    "        word = ngram[-1]\n",
    "        print(f\"  P({word}|{context}) = {prob:.4f}\")\n",
    "    \n",
    "    print(f\"  Total log probability: {log_prob:.4f}\")\n",
    "    print(f\"  Probability: {math.exp(log_prob):.6f}\")\n",
    "    print(f\"  Perplexity: {model.perplexity(example_sentence):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Combining N-grams with Naive Bayes\n",
    "\n",
    "### 5.1 N-gram Feature Naive Bayes Classifier\n",
    "\n",
    "Instead of using just unigrams (single words) as features, we can use bigrams and trigrams to capture word context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramNaiveBayes:\n",
    "    \"\"\"\n",
    "    Naive Bayes Classifier using N-gram features.\n",
    "    Combines unigrams, bigrams, and/or trigrams as features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ngram_range=(1, 2), k=1):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ngram_range : tuple (min_n, max_n)\n",
    "            Range of n-grams to use\n",
    "        k : float\n",
    "            Smoothing parameter\n",
    "        \"\"\"\n",
    "        self.ngram_range = ngram_range\n",
    "        self.k = k\n",
    "        self.class_priors = {}\n",
    "        self.feature_counts = {}  # {class: {feature: count}}\n",
    "        self.class_feature_totals = {}\n",
    "        self.vocabulary = set()  # All unique n-gram features\n",
    "        self.classes = []\n",
    "        \n",
    "    def _extract_features(self, text):\n",
    "        \"\"\"\n",
    "        Extract all n-gram features from text.\n",
    "        \"\"\"\n",
    "        tokens = preprocess_text(text)\n",
    "        features = []\n",
    "        \n",
    "        for n in range(self.ngram_range[0], self.ngram_range[1] + 1):\n",
    "            ngrams = extract_ngrams(tokens, n)\n",
    "            # Convert tuples to strings for easier handling\n",
    "            features.extend(['_'.join(ng) for ng in ngrams])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def fit(self, documents, labels):\n",
    "        \"\"\"\n",
    "        Train the classifier.\n",
    "        \"\"\"\n",
    "        self.classes = list(set(labels))\n",
    "        n_docs = len(documents)\n",
    "        \n",
    "        # Initialize\n",
    "        for c in self.classes:\n",
    "            self.feature_counts[c] = defaultdict(int)\n",
    "            self.class_feature_totals[c] = 0\n",
    "        \n",
    "        class_doc_counts = Counter(labels)\n",
    "        \n",
    "        # Count features\n",
    "        for doc, label in zip(documents, labels):\n",
    "            features = self._extract_features(doc)\n",
    "            for feature in features:\n",
    "                self.vocabulary.add(feature)\n",
    "                self.feature_counts[label][feature] += 1\n",
    "                self.class_feature_totals[label] += 1\n",
    "        \n",
    "        # Calculate priors\n",
    "        for c in self.classes:\n",
    "            self.class_priors[c] = class_doc_counts[c] / n_docs\n",
    "        \n",
    "        print(f\"N-gram Naive Bayes trained!\")\n",
    "        print(f\"N-gram range: {self.ngram_range}\")\n",
    "        print(f\"Total features (vocabulary): {len(self.vocabulary)}\")\n",
    "        print(f\"Class priors: {self.class_priors}\")\n",
    "        \n",
    "    def _feature_probability(self, feature, class_label):\n",
    "        \"\"\"\n",
    "        Calculate P(feature|class) with smoothing.\n",
    "        \"\"\"\n",
    "        count = self.feature_counts[class_label].get(feature, 0)\n",
    "        total = self.class_feature_totals[class_label]\n",
    "        vocab_size = len(self.vocabulary)\n",
    "        \n",
    "        return (count + self.k) / (total + self.k * vocab_size)\n",
    "    \n",
    "    def predict_proba(self, document):\n",
    "        \"\"\"\n",
    "        Calculate class probabilities.\n",
    "        \"\"\"\n",
    "        features = self._extract_features(document)\n",
    "        class_scores = {}\n",
    "        \n",
    "        for c in self.classes:\n",
    "            log_prob = math.log(self.class_priors[c])\n",
    "            for feature in features:\n",
    "                prob = self._feature_probability(feature, c)\n",
    "                log_prob += math.log(prob)\n",
    "            class_scores[c] = log_prob\n",
    "        \n",
    "        # Normalize\n",
    "        max_log = max(class_scores.values())\n",
    "        exp_scores = {c: math.exp(s - max_log) for c, s in class_scores.items()}\n",
    "        total = sum(exp_scores.values())\n",
    "        \n",
    "        return {c: s / total for c, s in exp_scores.items()}\n",
    "    \n",
    "    def predict(self, document):\n",
    "        \"\"\"\n",
    "        Predict class.\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(document)\n",
    "        return max(probs, key=probs.get)\n",
    "    \n",
    "    def evaluate(self, documents, labels):\n",
    "        \"\"\"\n",
    "        Evaluate classifier.\n",
    "        \"\"\"\n",
    "        predictions = [self.predict(doc) for doc in documents]\n",
    "        return {\n",
    "            'accuracy': accuracy_score(labels, predictions),\n",
    "            'precision': precision_score(labels, predictions, pos_label='positive'),\n",
    "            'recall': recall_score(labels, predictions, pos_label='positive'),\n",
    "            'f1_score': f1_score(labels, predictions, pos_label='positive'),\n",
    "            'predictions': predictions\n",
    "        }\n",
    "\n",
    "print(\"NGramNaiveBayes class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Comparing Unigram vs Bigram vs Combined Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended dataset for better comparison\n",
    "extended_training = training_data + [\n",
    "    (\"not bad service could be better\", \"positive\"),\n",
    "    (\"not good service could be worse\", \"negative\"),\n",
    "    (\"really great service highly recommend\", \"positive\"),\n",
    "    (\"really bad service do not recommend\", \"negative\"),\n",
    "    (\"food was not great disappointed\", \"negative\"),\n",
    "    (\"food was not bad enjoyed it\", \"positive\"),\n",
    "]\n",
    "\n",
    "extended_test = [\n",
    "    (\"great service excellent food\", \"positive\"),\n",
    "    (\"bad experience terrible service\", \"negative\"),\n",
    "    (\"not bad could be better\", \"positive\"),\n",
    "    (\"not good very disappointed\", \"negative\"),\n",
    "    (\"really great highly recommend\", \"positive\"),\n",
    "    (\"really bad do not recommend\", \"negative\"),\n",
    "]\n",
    "\n",
    "ext_train_docs = [t for t, l in extended_training]\n",
    "ext_train_labels = [l for t, l in extended_training]\n",
    "ext_test_docs = [t for t, l in extended_test]\n",
    "ext_test_labels = [l for t, l in extended_test]\n",
    "\n",
    "print(f\"Extended training set: {len(extended_training)} documents\")\n",
    "print(f\"Extended test set: {len(extended_test)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and compare different n-gram configurations\n",
    "print(\"Comparing N-gram Configurations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "configs = [\n",
    "    ((1, 1), \"Unigram only\"),\n",
    "    ((2, 2), \"Bigram only\"),\n",
    "    ((3, 3), \"Trigram only\"),\n",
    "    ((1, 2), \"Unigram + Bigram\"),\n",
    "    ((1, 3), \"Unigram + Bigram + Trigram\"),\n",
    "]\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for ngram_range, name in configs:\n",
    "    print(f\"\\nTraining: {name}\")\n",
    "    classifier = NGramNaiveBayes(ngram_range=ngram_range, k=1)\n",
    "    classifier.fit(ext_train_docs, ext_train_labels)\n",
    "    \n",
    "    results = classifier.evaluate(ext_test_docs, ext_test_labels)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Configuration': name,\n",
    "        'N-gram Range': str(ngram_range),\n",
    "        'Features': len(classifier.vocabulary),\n",
    "        'Accuracy': f\"{results['accuracy']:.3f}\",\n",
    "        'Precision': f\"{results['precision']:.3f}\",\n",
    "        'Recall': f\"{results['recall']:.3f}\",\n",
    "        'F1-Score': f\"{results['f1_score']:.3f}\",\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nComparison Summary:\")\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Metrics comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(comparison_results))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [float(r[metric]) for r in comparison_results]\n",
    "    axes[0].bar(x + i * width, values, width, label=metric)\n",
    "\n",
    "axes[0].set_xlabel('Configuration')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Classification Metrics by N-gram Configuration')\n",
    "axes[0].set_xticks(x + width * 1.5)\n",
    "axes[0].set_xticklabels([r['Configuration'] for r in comparison_results], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1.1)\n",
    "\n",
    "# Plot 2: Feature count vs F1-Score\n",
    "features = [r['Features'] for r in comparison_results]\n",
    "f1_scores = [float(r['F1-Score']) for r in comparison_results]\n",
    "configs_names = [r['Configuration'] for r in comparison_results]\n",
    "\n",
    "axes[1].scatter(features, f1_scores, s=100, c='steelblue')\n",
    "for i, txt in enumerate(configs_names):\n",
    "    axes[1].annotate(txt, (features[i], f1_scores[i]), \n",
    "                     textcoords=\"offset points\", xytext=(5, 5), fontsize=9)\n",
    "axes[1].set_xlabel('Number of Features')\n",
    "axes[1].set_ylabel('F1-Score')\n",
    "axes[1].set_title('Feature Count vs F1-Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ngram_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Demonstrating Why Bigrams Help: The \"Not\" Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classic \"not good\" vs \"not bad\" problem\n",
    "negation_examples = [\n",
    "    \"not good service\",\n",
    "    \"not bad service\",\n",
    "    \"good service\",\n",
    "    \"bad service\",\n",
    "]\n",
    "\n",
    "print(\"Demonstrating the Negation Problem:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nUnigram features cannot capture negation context!\")\n",
    "\n",
    "# Show features extracted\n",
    "print(\"\\nUnigram vs Bigram Features:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for example in negation_examples:\n",
    "    tokens = preprocess_text(example)\n",
    "    unigrams = ['_'.join(ng) for ng in extract_ngrams(tokens, 1)]\n",
    "    bigrams = ['_'.join(ng) for ng in extract_ngrams(tokens, 2)]\n",
    "    \n",
    "    print(f\"\\n'{example}'\")\n",
    "    print(f\"  Unigrams: {unigrams}\")\n",
    "    print(f\"  Bigrams:  {bigrams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models and compare predictions\n",
    "print(\"\\nPrediction Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train unigram-only model\n",
    "unigram_nb = NGramNaiveBayes(ngram_range=(1, 1), k=1)\n",
    "unigram_nb.fit(ext_train_docs, ext_train_labels)\n",
    "\n",
    "# Train unigram+bigram model\n",
    "bigram_nb = NGramNaiveBayes(ngram_range=(1, 2), k=1)\n",
    "bigram_nb.fit(ext_train_docs, ext_train_labels)\n",
    "\n",
    "print(\"\\nTest sentences with negation:\")\n",
    "test_negation = [\n",
    "    (\"not bad service\", \"positive\"),  # Negation of negative = positive\n",
    "    (\"not good service\", \"negative\"),  # Negation of positive = negative\n",
    "]\n",
    "\n",
    "for text, true_label in test_negation:\n",
    "    uni_pred = unigram_nb.predict(text)\n",
    "    uni_prob = unigram_nb.predict_proba(text)\n",
    "    \n",
    "    bi_pred = bigram_nb.predict(text)\n",
    "    bi_prob = bigram_nb.predict_proba(text)\n",
    "    \n",
    "    print(f\"\\n'{text}' (True: {true_label})\")\n",
    "    print(f\"  Unigram:        {uni_pred:8s} (pos={uni_prob['positive']:.3f}, neg={uni_prob['negative']:.3f})\")\n",
    "    print(f\"  Unigram+Bigram: {bi_pred:8s} (pos={bi_prob['positive']:.3f}, neg={bi_prob['negative']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Class-Specific N-gram Language Models for Classification\n",
    "\n",
    "Another approach: Train separate language models for each class and use perplexity for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelClassifier:\n",
    "    \"\"\"\n",
    "    Classifier using class-specific language models.\n",
    "    Classifies based on which class's LM assigns lower perplexity.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n=2, k=1):\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.class_models = {}  # {class: NGramLanguageModel}\n",
    "        self.classes = []\n",
    "        \n",
    "    def fit(self, documents, labels):\n",
    "        \"\"\"\n",
    "        Train a separate language model for each class.\n",
    "        \"\"\"\n",
    "        self.classes = list(set(labels))\n",
    "        \n",
    "        # Group documents by class\n",
    "        class_docs = defaultdict(list)\n",
    "        for doc, label in zip(documents, labels):\n",
    "            class_docs[label].append(doc)\n",
    "        \n",
    "        # Train LM for each class\n",
    "        for c in self.classes:\n",
    "            print(f\"\\nTraining {self.n}-gram LM for class '{c}':\")\n",
    "            self.class_models[c] = NGramLanguageModel(n=self.n, k=self.k)\n",
    "            self.class_models[c].fit(class_docs[c])\n",
    "    \n",
    "    def predict(self, document):\n",
    "        \"\"\"\n",
    "        Predict using perplexity: lower perplexity = more likely class.\n",
    "        \"\"\"\n",
    "        perplexities = {}\n",
    "        for c in self.classes:\n",
    "            perplexities[c] = self.class_models[c].perplexity(document)\n",
    "        \n",
    "        # Return class with lowest perplexity\n",
    "        return min(perplexities, key=perplexities.get)\n",
    "    \n",
    "    def predict_with_scores(self, document):\n",
    "        \"\"\"\n",
    "        Return prediction with perplexity scores.\n",
    "        \"\"\"\n",
    "        perplexities = {}\n",
    "        for c in self.classes:\n",
    "            perplexities[c] = self.class_models[c].perplexity(document)\n",
    "        \n",
    "        prediction = min(perplexities, key=perplexities.get)\n",
    "        return prediction, perplexities\n",
    "\n",
    "print(\"LanguageModelClassifier class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LM-based classifier\n",
    "print(\"Training Language Model Classifier:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lm_classifier = LanguageModelClassifier(n=2, k=1)\n",
    "lm_classifier.fit(ext_train_docs, ext_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the LM classifier\n",
    "print(\"\\nLanguage Model Classifier Predictions:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for text, true_label in extended_test:\n",
    "    pred, perplexities = lm_classifier.predict_with_scores(text)\n",
    "    status = \"âœ“\" if pred == true_label else \"âœ—\"\n",
    "    \n",
    "    print(f\"\\n{status} '{text}'\")\n",
    "    print(f\"   True: {true_label}, Predicted: {pred}\")\n",
    "    print(f\"   Perplexity - positive: {perplexities['positive']:.2f}, negative: {perplexities['negative']:.2f}\")\n",
    "    print(f\"   (Lower perplexity = more similar to class language)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Using Sklearn for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with sklearn's implementation\n",
    "print(\"Sklearn Naive Bayes Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sklearn_results = []\n",
    "\n",
    "for ngram_range, name in [((1, 1), \"Unigram\"), ((1, 2), \"Unigram+Bigram\"), ((1, 3), \"Uni+Bi+Tri\")]:\n",
    "    # Create vectorizer\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    \n",
    "    # Transform data\n",
    "    X_train = vectorizer.fit_transform(ext_train_docs)\n",
    "    X_test = vectorizer.transform(ext_test_docs)\n",
    "    \n",
    "    # Train classifier\n",
    "    clf = MultinomialNB(alpha=1.0)  # alpha=1 is Laplace smoothing\n",
    "    clf.fit(X_train, ext_train_labels)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    sklearn_results.append({\n",
    "        'Configuration': name,\n",
    "        'Features': len(vectorizer.vocabulary_),\n",
    "        'Accuracy': f\"{accuracy_score(ext_test_labels, y_pred):.3f}\",\n",
    "        'F1-Score': f\"{f1_score(ext_test_labels, y_pred, pos_label='positive'):.3f}\",\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Features: {len(vectorizer.vocabulary_)}\")\n",
    "    print(f\"  Sample features: {list(vectorizer.vocabulary_.keys())[:10]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nSklearn Results Summary:\")\n",
    "print(pd.DataFrame(sklearn_results).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Exercises\n",
    "\n",
    "### Exercise 1: Implement Add-k Smoothing Comparison\n",
    "\n",
    "Compare classifier performance with different smoothing values (k=0.1, 0.5, 1.0, 2.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: Create classifiers with different k values and compare their F1 scores\n",
    "\n",
    "smoothing_values = [0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "# TODO: Implement comparison\n",
    "# for k in smoothing_values:\n",
    "#     classifier = NGramNaiveBayes(ngram_range=(1, 2), k=k)\n",
    "#     ...\n",
    "\n",
    "print(\"Exercise 1: Implement smoothing comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a Nepali Sentiment Dataset\n",
    "\n",
    "Create a small sentiment analysis dataset using Nepali business reviews and test the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Create Nepali sentiment data (you can use romanized Nepali)\n",
    "\n",
    "nepali_data = [\n",
    "    # Add your Nepali examples here\n",
    "    # (\"ramro sewa\", \"positive\"),\n",
    "    # (\"naramro khana\", \"negative\"),\n",
    "]\n",
    "\n",
    "print(\"Exercise 2: Create Nepali sentiment dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Perplexity Analysis\n",
    "\n",
    "Calculate and compare the perplexity of different sentences using your trained language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Test sentences:\n",
    "test_sentences_ex3 = [\n",
    "    \"I love learning new things\",\n",
    "    \"machine learning is fun\",\n",
    "    \"random words here there\",\n",
    "    \"the quick brown fox\",\n",
    "]\n",
    "\n",
    "# TODO: Calculate perplexity for each sentence using bigram_model\n",
    "# Which sentences have lower perplexity and why?\n",
    "\n",
    "print(\"Exercise 3: Analyze perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Feature Importance Analysis\n",
    "\n",
    "Find the most discriminative n-gram features for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n",
    "# Hint: Compare P(feature|positive) vs P(feature|negative)\n",
    "# Features with largest difference are most discriminative\n",
    "\n",
    "def find_discriminative_features(classifier, top_n=10):\n",
    "    \"\"\"\n",
    "    Find the most discriminative features for each class.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "print(\"Exercise 4: Find discriminative features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Naive Bayes** is a simple but effective probabilistic classifier based on Bayes' theorem\n",
    "\n",
    "2. **Laplace Smoothing** prevents zero probabilities for unseen words\n",
    "\n",
    "3. **N-gram Language Models** capture word context:\n",
    "   - Unigrams: No context (bag of words)\n",
    "   - Bigrams: One word context\n",
    "   - Trigrams: Two words context\n",
    "\n",
    "4. **Combining N-grams with Naive Bayes** improves classification by:\n",
    "   - Capturing word order and phrases\n",
    "   - Handling negation better (\"not good\" vs \"good\")\n",
    "   - Providing richer feature representation\n",
    "\n",
    "5. **Trade-offs**:\n",
    "   - Higher n-grams = more context but more sparsity\n",
    "   - More features = potentially better performance but risk of overfitting\n",
    "\n",
    "### Further Reading:\n",
    "- Jurafsky & Martin, \"Speech and Language Processing\" - Chapters on Naive Bayes and N-grams\n",
    "- Manning et al., \"Introduction to Information Retrieval\" - Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lab completed! Great work! ðŸŽ‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
