{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š Word Embeddings: Word2Vec and GloVe Tutorial\n",
    "\n",
    "This notebook provides a comprehensive guide to understanding and implementing **Word2Vec** and **GloVe** word embeddings.\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction to Word Embeddings\n",
    "2. Word2Vec: Theory and Implementation\n",
    "   - Skip-gram Model\n",
    "   - CBOW Model\n",
    "3. GloVe: Theory and Implementation\n",
    "4. Comparing Word2Vec and GloVe\n",
    "5. Practical Applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install gensim==4.3.2 nltk matplotlib numpy scikit-learn --quiet\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Gensim version: {gensim.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Introduction to Word Embeddings\n",
    "\n",
    "### Why Word Embeddings?\n",
    "\n",
    "Traditional text representations like **One-Hot Encoding** and **Bag of Words** have limitations:\n",
    "- High dimensionality (vocabulary size)\n",
    "- No semantic meaning captured\n",
    "- No relationship between words\n",
    "\n",
    "**Word Embeddings** solve these problems by:\n",
    "- Representing words as dense vectors (typically 50-300 dimensions)\n",
    "- Capturing semantic relationships\n",
    "- Similar words have similar vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: One-Hot Encoding vs Word Embeddings\n",
    "\n",
    "vocabulary = [\"king\", \"queen\", \"man\", \"woman\", \"child\"]\n",
    "\n",
    "# One-Hot Encoding\n",
    "print(\"=\" * 50)\n",
    "print(\"ONE-HOT ENCODING\")\n",
    "print(\"=\" * 50)\n",
    "for i, word in enumerate(vocabulary):\n",
    "    one_hot = [0] * len(vocabulary)\n",
    "    one_hot[i] = 1\n",
    "    print(f\"{word:8} -> {one_hot}\")\n",
    "\n",
    "print(\"\\nâŒ Problems:\")\n",
    "print(\"   - All words are equally distant\")\n",
    "print(\"   - No semantic meaning\")\n",
    "print(\"   - Sparse vectors (mostly zeros)\")\n",
    "\n",
    "# Word Embeddings (hypothetical example)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"WORD EMBEDDINGS (Dense Vectors)\")\n",
    "print(\"=\" * 50)\n",
    "embeddings = {\n",
    "    \"king\":   [0.8, 0.6, 0.9, 0.1],\n",
    "    \"queen\":  [0.7, 0.6, 0.1, 0.9],\n",
    "    \"man\":    [0.5, 0.4, 0.8, 0.2],\n",
    "    \"woman\":  [0.4, 0.4, 0.2, 0.8],\n",
    "    \"child\":  [0.3, 0.2, 0.5, 0.5]\n",
    "}\n",
    "for word, vec in embeddings.items():\n",
    "    print(f\"{word:8} -> {vec}\")\n",
    "\n",
    "print(\"\\nâœ… Benefits:\")\n",
    "print(\"   - Dense, low-dimensional\")\n",
    "print(\"   - Similar words have similar vectors\")\n",
    "print(\"   - Captures semantic relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Word2Vec: Theory\n",
    "\n",
    "**Word2Vec** was developed by Mikolov et al. at Google (2013). It learns word embeddings by predicting context.\n",
    "\n",
    "### Key Idea: \"You shall know a word by the company it keeps\" - J.R. Firth\n",
    "\n",
    "### Two Architectures:\n",
    "\n",
    "#### 1. Skip-gram\n",
    "- **Input**: Center word\n",
    "- **Output**: Context words\n",
    "- Better for rare words\n",
    "- Slower training\n",
    "\n",
    "#### 2. CBOW (Continuous Bag of Words)\n",
    "- **Input**: Context words\n",
    "- **Output**: Center word\n",
    "- Better for frequent words\n",
    "- Faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Skip-gram vs CBOW\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "words = sentence.lower().split()\n",
    "window_size = 2\n",
    "\n",
    "print(\"Sentence:\", sentence)\n",
    "print(f\"Window size: {window_size}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Skip-gram examples\n",
    "print(\"\\nğŸ“˜ SKIP-GRAM: Predict context from center word\")\n",
    "print(\"-\" * 60)\n",
    "center_idx = 4  # \"jumps\"\n",
    "center_word = words[center_idx]\n",
    "context = words[max(0, center_idx-window_size):center_idx] + \\\n",
    "          words[center_idx+1:center_idx+window_size+1]\n",
    "\n",
    "print(f\"Center word: '{center_word}'\")\n",
    "print(f\"Context words: {context}\")\n",
    "print(f\"\\nTraining pairs (input -> output):\")\n",
    "for ctx in context:\n",
    "    print(f\"   {center_word} -> {ctx}\")\n",
    "\n",
    "# CBOW examples\n",
    "print(\"\\nğŸ“— CBOW: Predict center from context words\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Context words: {context}\")\n",
    "print(f\"Target word: '{center_word}'\")\n",
    "print(f\"\\nTraining pair:\")\n",
    "print(f\"   {context} -> {center_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Architecture Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual representation of Word2Vec architecture\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Skip-gram\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Skip-gram Model', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Input layer\n",
    "ax1.add_patch(plt.Rectangle((1, 4), 2, 2, fill=True, color='lightblue', ec='black'))\n",
    "ax1.text(2, 5, 'Input\\n(Center)', ha='center', va='center', fontsize=10)\n",
    "ax1.text(2, 3.3, '\"jumps\"', ha='center', va='center', fontsize=9, style='italic')\n",
    "\n",
    "# Hidden layer\n",
    "ax1.add_patch(plt.Rectangle((4, 4), 2, 2, fill=True, color='lightgreen', ec='black'))\n",
    "ax1.text(5, 5, 'Hidden\\nLayer', ha='center', va='center', fontsize=10)\n",
    "ax1.text(5, 3.3, 'Word Vector', ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Output layer\n",
    "for i, word in enumerate(['brown', 'fox', 'over', 'the']):\n",
    "    y = 7.5 - i * 2\n",
    "    ax1.add_patch(plt.Rectangle((7, y-0.4), 2, 0.8, fill=True, color='lightyellow', ec='black'))\n",
    "    ax1.text(8, y, f'\"{word}\"', ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Arrows\n",
    "ax1.annotate('', xy=(4, 5), xytext=(3, 5), arrowprops=dict(arrowstyle='->', color='black'))\n",
    "for i in range(4):\n",
    "    y = 7.5 - i * 2\n",
    "    ax1.annotate('', xy=(7, y), xytext=(6, 5), arrowprops=dict(arrowstyle='->', color='black', alpha=0.5))\n",
    "\n",
    "# CBOW\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.axis('off')\n",
    "ax2.set_title('CBOW Model', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Input layer (context words)\n",
    "for i, word in enumerate(['brown', 'fox', 'over', 'the']):\n",
    "    y = 7.5 - i * 2\n",
    "    ax2.add_patch(plt.Rectangle((1, y-0.4), 2, 0.8, fill=True, color='lightblue', ec='black'))\n",
    "    ax2.text(2, y, f'\"{word}\"', ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Hidden layer\n",
    "ax2.add_patch(plt.Rectangle((4, 4), 2, 2, fill=True, color='lightgreen', ec='black'))\n",
    "ax2.text(5, 5, 'Hidden\\n(Average)', ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Output layer\n",
    "ax2.add_patch(plt.Rectangle((7, 4), 2, 2, fill=True, color='lightyellow', ec='black'))\n",
    "ax2.text(8, 5, 'Output\\n(Target)', ha='center', va='center', fontsize=10)\n",
    "ax2.text(8, 3.3, '\"jumps\"', ha='center', va='center', fontsize=9, style='italic')\n",
    "\n",
    "# Arrows\n",
    "for i in range(4):\n",
    "    y = 7.5 - i * 2\n",
    "    ax2.annotate('', xy=(4, 5), xytext=(3, y), arrowprops=dict(arrowstyle='->', color='black', alpha=0.5))\n",
    "ax2.annotate('', xy=(7, 5), xytext=(6, 5), arrowprops=dict(arrowstyle='->', color='black'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('word2vec_architecture.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nğŸ’¡ Key Difference:\")\n",
    "print(\"   Skip-gram: 1 center word â†’ multiple context words\")\n",
    "print(\"   CBOW: multiple context words â†’ 1 center word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Training Word2Vec from Scratch (Simplified)\n",
    "\n",
    "Let's implement a simplified version to understand the mechanics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleWord2Vec:\n",
    "    \"\"\"\n",
    "    Simplified Word2Vec implementation for educational purposes.\n",
    "    Uses Skip-gram with negative sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences, embedding_dim=50, window_size=2, learning_rate=0.01):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Build vocabulary\n",
    "        self.word_counts = Counter(word for sent in sentences for word in sent)\n",
    "        self.vocab = list(self.word_counts.keys())\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        # Word to index mapping\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "\n",
    "        # Initialize embeddings randomly\n",
    "        self.W_input = np.random.randn(self.vocab_size, embedding_dim) * 0.01\n",
    "        self.W_output = np.random.randn(embedding_dim, self.vocab_size) * 0.01\n",
    "\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "    def generate_training_data(self, sentences):\n",
    "        \"\"\"Generate (center, context) pairs for Skip-gram.\"\"\"\n",
    "        training_data = []\n",
    "        for sentence in sentences:\n",
    "            for i, center_word in enumerate(sentence):\n",
    "                # Get context words within window\n",
    "                start = max(0, i - self.window_size)\n",
    "                end = min(len(sentence), i + self.window_size + 1)\n",
    "\n",
    "                for j in range(start, end):\n",
    "                    if i != j:\n",
    "                        context_word = sentence[j]\n",
    "                        training_data.append((center_word, context_word))\n",
    "\n",
    "        return training_data\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "    def train_step(self, center_word, context_word):\n",
    "        \"\"\"Single training step with gradient descent.\"\"\"\n",
    "        center_idx = self.word2idx[center_word]\n",
    "        context_idx = self.word2idx[context_word]\n",
    "\n",
    "        # Forward pass\n",
    "        hidden = self.W_input[center_idx]  # (embedding_dim,)\n",
    "        output = np.dot(hidden, self.W_output)  # (vocab_size,)\n",
    "        probs = self.sigmoid(output)\n",
    "\n",
    "        # Create target (1 for context word, 0 for others)\n",
    "        target = np.zeros(self.vocab_size)\n",
    "        target[context_idx] = 1\n",
    "\n",
    "        # Backward pass\n",
    "        error = probs - target\n",
    "\n",
    "        # Update output weights\n",
    "        self.W_output -= self.learning_rate * np.outer(hidden, error)\n",
    "\n",
    "        # Update input weights\n",
    "        grad_hidden = np.dot(self.W_output, error)\n",
    "        self.W_input[center_idx] -= self.learning_rate * grad_hidden\n",
    "\n",
    "        # Return loss\n",
    "        loss = -np.log(probs[context_idx] + 1e-10)\n",
    "        return loss\n",
    "\n",
    "    def train(self, sentences, epochs=100):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        training_data = self.generate_training_data(sentences)\n",
    "        print(f\"Training pairs: {len(training_data)}\")\n",
    "\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            np.random.shuffle(training_data)\n",
    "\n",
    "            for center, context in training_data:\n",
    "                loss = self.train_step(center, context)\n",
    "                total_loss += loss\n",
    "\n",
    "            avg_loss = total_loss / len(training_data)\n",
    "            losses.append(avg_loss)\n",
    "\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"Get embedding vector for a word.\"\"\"\n",
    "        if word in self.word2idx:\n",
    "            return self.W_input[self.word2idx[word]]\n",
    "        return None\n",
    "\n",
    "    def most_similar(self, word, top_n=5):\n",
    "        \"\"\"Find most similar words using cosine similarity.\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            return []\n",
    "\n",
    "        word_vec = self.get_embedding(word).reshape(1, -1)\n",
    "        similarities = cosine_similarity(word_vec, self.W_input)[0]\n",
    "\n",
    "        # Get top similar words (excluding the word itself)\n",
    "        similar_idx = np.argsort(similarities)[::-1][1:top_n+1]\n",
    "        return [(self.idx2word[idx], similarities[idx]) for idx in similar_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our simple Word2Vec on sample data\n",
    "\n",
    "# Sample corpus\n",
    "sample_corpus = [\n",
    "    \"the king rules the kingdom\",\n",
    "    \"the queen rules the kingdom\",\n",
    "    \"the king and queen are royal\",\n",
    "    \"man and woman are human\",\n",
    "    \"the prince is the son of king\",\n",
    "    \"the princess is the daughter of queen\",\n",
    "    \"king is a man who rules\",\n",
    "    \"queen is a woman who rules\",\n",
    "    \"boy and girl are young\",\n",
    "    \"man was once a boy\",\n",
    "    \"woman was once a girl\"\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "sentences = [sent.lower().split() for sent in sample_corpus]\n",
    "print(\"Sample sentences:\")\n",
    "for sent in sentences[:3]:\n",
    "    print(f\"  {sent}\")\n",
    "\n",
    "# Train model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Simple Word2Vec...\")\n",
    "print(\"=\"*50)\n",
    "simple_w2v = SimpleWord2Vec(sentences, embedding_dim=20, window_size=2)\n",
    "losses = simple_w2v.train(sentences, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Test similar words\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Finding Similar Words\")\n",
    "print(\"=\"*50)\n",
    "for test_word in ['king', 'queen', 'man', 'woman']:\n",
    "    similar = simple_w2v.most_similar(test_word, top_n=3)\n",
    "    print(f\"\\n'{test_word}' is similar to:\")\n",
    "    for word, score in similar:\n",
    "        print(f\"   {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Using Gensim's Word2Vec (Production-Ready)\n",
    "\n",
    "Now let's use Gensim's optimized implementation with a larger corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Brown corpus (1 million+ words)\n",
    "print(\"Loading Brown corpus...\")\n",
    "brown_sentences = brown.sents()\n",
    "\n",
    "# Preprocess: lowercase\n",
    "processed_sentences = [[word.lower() for word in sent] for sent in brown_sentences]\n",
    "\n",
    "print(f\"Number of sentences: {len(processed_sentences):,}\")\n",
    "print(f\"Sample sentence: {processed_sentences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec using Gensim\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Word2Vec with Gensim (Skip-gram)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Skip-gram model (sg=1)\n",
    "w2v_skipgram = Word2Vec(\n",
    "    sentences=processed_sentences,\n",
    "    vector_size=100,      # Embedding dimension\n",
    "    window=5,             # Context window size\n",
    "    min_count=5,          # Ignore words with freq < 5\n",
    "    workers=4,            # Parallel training threads\n",
    "    sg=1,                 # 1 = Skip-gram, 0 = CBOW\n",
    "    epochs=10             # Training iterations\n",
    ")\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(w2v_skipgram.wv):,}\")\n",
    "print(f\"Embedding dimension: {w2v_skipgram.wv.vector_size}\")\n",
    "\n",
    "# Train CBOW model for comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Word2Vec with Gensim (CBOW)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "w2v_cbow = Word2Vec(\n",
    "    sentences=processed_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=4,\n",
    "    sg=0,                 # CBOW\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(w2v_cbow.wv):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Word2Vec embeddings\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Exploring Word2Vec Embeddings\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Similar words\n",
    "print(\"\\nğŸ“Œ Most Similar Words (Skip-gram):\")\n",
    "test_words = ['king', 'woman', 'computer', 'money']\n",
    "\n",
    "for word in test_words:\n",
    "    if word in w2v_skipgram.wv:\n",
    "        similar = w2v_skipgram.wv.most_similar(word, topn=5)\n",
    "        print(f\"\\n'{word}':\")\n",
    "        for sim_word, score in similar:\n",
    "            print(f\"   {sim_word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Word Analogies: king - man + woman = ?\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Word Analogies\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analogies = [\n",
    "    ('king', 'man', 'woman'),      # king - man + woman = queen?\n",
    "    ('paris', 'france', 'italy'),  # paris - france + italy = rome?\n",
    "    ('good', 'better', 'bad'),     # good - better + bad = worse?\n",
    "]\n",
    "\n",
    "print(\"\\nAnalogy: A is to B as C is to ?\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for a, b, c in analogies:\n",
    "    try:\n",
    "        # Find: a - b + c = ?\n",
    "        result = w2v_skipgram.wv.most_similar(\n",
    "            positive=[a, c],\n",
    "            negative=[b],\n",
    "            topn=3\n",
    "        )\n",
    "        print(f\"\\n{a} - {b} + {c} = ?\")\n",
    "        for word, score in result:\n",
    "            print(f\"   {word}: {score:.4f}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"\\n{a} - {b} + {c}: Word not in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Word similarity score\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Word Similarity Scores\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "word_pairs = [\n",
    "    ('man', 'woman'),\n",
    "    ('king', 'queen'),\n",
    "    ('good', 'bad'),\n",
    "    ('car', 'automobile'),\n",
    "    ('happy', 'sad')\n",
    "]\n",
    "\n",
    "print(\"\\nCosine Similarity between word pairs:\")\n",
    "print(\"-\" * 40)\n",
    "for w1, w2 in word_pairs:\n",
    "    try:\n",
    "        sim = w2v_skipgram.wv.similarity(w1, w2)\n",
    "        print(f\"{w1:12} <-> {w2:12}: {sim:.4f}\")\n",
    "    except KeyError:\n",
    "        print(f\"{w1:12} <-> {w2:12}: Word not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Odd one out\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Find the Odd One Out\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "word_lists = [\n",
    "    ['breakfast', 'lunch', 'dinner', 'computer'],\n",
    "    ['man', 'woman', 'child', 'car'],\n",
    "    ['red', 'blue', 'green', 'happy']\n",
    "]\n",
    "\n",
    "for words in word_lists:\n",
    "    try:\n",
    "        odd = w2v_skipgram.wv.doesnt_match(words)\n",
    "        print(f\"\\nWords: {words}\")\n",
    "        print(f\"Odd one out: '{odd}'\")\n",
    "    except KeyError:\n",
    "        print(f\"\\nWords: {words} - Some words not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. GloVe: Theory\n",
    "\n",
    "**GloVe (Global Vectors)** was developed by Pennington et al. at Stanford (2014).\n",
    "\n",
    "### Key Differences from Word2Vec:\n",
    "\n",
    "| Aspect | Word2Vec | GloVe |\n",
    "|--------|----------|-------|\n",
    "| Approach | Predictive (neural network) | Count-based + Predictive |\n",
    "| Training | Local context windows | Global co-occurrence matrix |\n",
    "| Objective | Predict context/center | Minimize reconstruction error |\n",
    "\n",
    "### GloVe's Key Insight:\n",
    "\n",
    "The ratio of co-occurrence probabilities encodes meaning:\n",
    "\n",
    "$$\\frac{P(k|ice)}{P(k|steam)} = \\begin{cases} \\text{large} & \\text{if } k = \\text{\"solid\"} \\\\ \\text{small} & \\text{if } k = \\text{\"gas\"} \\\\ \\approx 1 & \\text{if } k = \\text{\"water\"} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate GloVe's co-occurrence concept\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GloVe: Co-occurrence Matrix Concept\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_text = \"\"\"\n",
    "The king sat on the throne. The queen stood beside the king.\n",
    "The throne was golden. The king wore a crown.\n",
    "The queen also wore a crown. The crown was jeweled.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and build co-occurrence matrix\n",
    "words = sample_text.lower().split()\n",
    "words = [w.strip('.,') for w in words if w.strip('.,')]\n",
    "\n",
    "vocab = list(set(words))\n",
    "vocab_size = len(vocab)\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "# Build co-occurrence matrix (window=2)\n",
    "cooccur = np.zeros((vocab_size, vocab_size))\n",
    "window = 2\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    for j in range(max(0, i-window), min(len(words), i+window+1)):\n",
    "        if i != j:\n",
    "            cooccur[word2idx[word]][word2idx[words[j]]] += 1\n",
    "\n",
    "# Display subset of co-occurrence matrix\n",
    "display_words = ['king', 'queen', 'throne', 'crown', 'golden']\n",
    "display_idx = [word2idx[w] for w in display_words if w in word2idx]\n",
    "display_words = [vocab[i] for i in display_idx]\n",
    "\n",
    "print(\"\\nCo-occurrence Matrix (subset):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Print header\n",
    "header = \"         \" + \" \".join([f\"{w:>8}\" for w in display_words])\n",
    "print(header)\n",
    "\n",
    "# Print rows\n",
    "for i, word in enumerate(display_words):\n",
    "    row = [cooccur[word2idx[word]][word2idx[w]] for w in display_words]\n",
    "    row_str = f\"{word:>8} \" + \" \".join([f\"{int(v):>8}\" for v in row])\n",
    "    print(row_str)\n",
    "\n",
    "print(\"\\nğŸ’¡ GloVe learns embeddings by factorizing this matrix!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified GloVe implementation\n",
    "\n",
    "class SimpleGloVe:\n",
    "    \"\"\"\n",
    "    Simplified GloVe implementation for educational purposes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences, embedding_dim=50, window_size=2, x_max=100, alpha=0.75):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.x_max = x_max\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Build vocabulary\n",
    "        all_words = [word for sent in sentences for word in sent]\n",
    "        self.word_counts = Counter(all_words)\n",
    "        self.vocab = list(self.word_counts.keys())\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.word2idx = {w: i for i, w in enumerate(self.vocab)}\n",
    "        self.idx2word = {i: w for w, i in self.word2idx.items()}\n",
    "\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "\n",
    "        # Build co-occurrence matrix\n",
    "        self.cooccur = self._build_cooccurrence(sentences)\n",
    "\n",
    "        # Initialize embeddings\n",
    "        self.W = np.random.randn(self.vocab_size, embedding_dim) * 0.01\n",
    "        self.W_context = np.random.randn(self.vocab_size, embedding_dim) * 0.01\n",
    "        self.b = np.zeros(self.vocab_size)\n",
    "        self.b_context = np.zeros(self.vocab_size)\n",
    "\n",
    "    def _build_cooccurrence(self, sentences):\n",
    "        \"\"\"Build co-occurrence matrix with distance weighting.\"\"\"\n",
    "        cooccur = defaultdict(float)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for i, center in enumerate(sentence):\n",
    "                for j in range(max(0, i-self.window_size), min(len(sentence), i+self.window_size+1)):\n",
    "                    if i != j:\n",
    "                        context = sentence[j]\n",
    "                        distance = abs(i - j)\n",
    "                        # Weight by inverse distance\n",
    "                        cooccur[(self.word2idx[center], self.word2idx[context])] += 1.0 / distance\n",
    "\n",
    "        print(f\"Non-zero co-occurrences: {len(cooccur):,}\")\n",
    "        return cooccur\n",
    "\n",
    "    def weighting_function(self, x):\n",
    "        \"\"\"GloVe weighting function to reduce impact of very frequent co-occurrences.\"\"\"\n",
    "        if x < self.x_max:\n",
    "            return (x / self.x_max) ** self.alpha\n",
    "        return 1.0\n",
    "\n",
    "    def train(self, epochs=50, learning_rate=0.05):\n",
    "        \"\"\"Train GloVe embeddings.\"\"\"\n",
    "        print(f\"\\nTraining GloVe for {epochs} epochs...\")\n",
    "        losses = []\n",
    "\n",
    "        # Get training pairs\n",
    "        pairs = list(self.cooccur.items())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            np.random.shuffle(pairs)\n",
    "\n",
    "            for (i, j), x_ij in pairs:\n",
    "                if x_ij == 0:\n",
    "                    continue\n",
    "\n",
    "                # Compute cost\n",
    "                weight = self.weighting_function(x_ij)\n",
    "\n",
    "                # Predicted log co-occurrence\n",
    "                pred = np.dot(self.W[i], self.W_context[j]) + self.b[i] + self.b_context[j]\n",
    "\n",
    "                # Difference from actual log co-occurrence\n",
    "                diff = pred - np.log(x_ij)\n",
    "\n",
    "                # Weighted squared error\n",
    "                loss = weight * diff ** 2\n",
    "                total_loss += loss\n",
    "\n",
    "                # Gradients\n",
    "                grad = weight * diff\n",
    "\n",
    "                # Update embeddings\n",
    "                self.W[i] -= learning_rate * grad * self.W_context[j]\n",
    "                self.W_context[j] -= learning_rate * grad * self.W[i]\n",
    "                self.b[i] -= learning_rate * grad\n",
    "                self.b_context[j] -= learning_rate * grad\n",
    "\n",
    "            avg_loss = total_loss / len(pairs)\n",
    "            losses.append(avg_loss)\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Final embeddings: average of word and context vectors\n",
    "        self.embeddings = (self.W + self.W_context) / 2\n",
    "        return losses\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        if word in self.word2idx:\n",
    "            return self.embeddings[self.word2idx[word]]\n",
    "        return None\n",
    "\n",
    "    def most_similar(self, word, top_n=5):\n",
    "        if word not in self.word2idx:\n",
    "            return []\n",
    "\n",
    "        word_vec = self.get_embedding(word).reshape(1, -1)\n",
    "        similarities = cosine_similarity(word_vec, self.embeddings)[0]\n",
    "\n",
    "        similar_idx = np.argsort(similarities)[::-1][1:top_n+1]\n",
    "        return [(self.idx2word[idx], similarities[idx]) for idx in similar_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train simplified GloVe\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Training Simplified GloVe\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use same sample corpus\n",
    "glove_model = SimpleGloVe(sentences, embedding_dim=20, window_size=2)\n",
    "glove_losses = glove_model.train(epochs=100, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GloVe training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(glove_losses, 'g-', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('GloVe Training Loss', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find similar words\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GloVe Similar Words\")\n",
    "print(\"=\"*50)\n",
    "for test_word in ['king', 'queen', 'man', 'woman']:\n",
    "    similar = glove_model.most_similar(test_word, top_n=3)\n",
    "    print(f\"\\n'{test_word}' is similar to:\")\n",
    "    for word, score in similar:\n",
    "        print(f\"   {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Using Pre-trained GloVe Embeddings\n",
    "\n",
    "Let's download and use Stanford's pre-trained GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained GloVe embeddings (50d version for speed)\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "glove_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "glove_dir = \"glove\"\n",
    "glove_file = \"glove.6B.50d.txt\"\n",
    "\n",
    "# Create directory\n",
    "os.makedirs(glove_dir, exist_ok=True)\n",
    "\n",
    "# Check if file exists\n",
    "glove_path = os.path.join(glove_dir, glove_file)\n",
    "\n",
    "if not os.path.exists(glove_path):\n",
    "    print(\"Downloading GloVe embeddings (this may take a few minutes)...\")\n",
    "    zip_path = os.path.join(glove_dir, \"glove.6B.zip\")\n",
    "\n",
    "    # Download\n",
    "    urllib.request.urlretrieve(glove_url, zip_path)\n",
    "\n",
    "    # Extract only the 50d file\n",
    "    print(\"Extracting...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extract(glove_file, glove_dir)\n",
    "\n",
    "    # Clean up zip\n",
    "    os.remove(zip_path)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"GloVe embeddings already downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "\n",
    "def load_glove(filepath):\n",
    "    \"\"\"Load GloVe embeddings from file.\"\"\"\n",
    "    embeddings = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "glove_embeddings = load_glove(glove_path)\n",
    "print(f\"Loaded {len(glove_embeddings):,} word vectors\")\n",
    "print(f\"Embedding dimension: {len(list(glove_embeddings.values())[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for GloVe\n",
    "\n",
    "def glove_similarity(word1, word2, embeddings):\n",
    "    \"\"\"Calculate cosine similarity between two words.\"\"\"\n",
    "    if word1 not in embeddings or word2 not in embeddings:\n",
    "        return None\n",
    "    vec1 = embeddings[word1].reshape(1, -1)\n",
    "    vec2 = embeddings[word2].reshape(1, -1)\n",
    "    return cosine_similarity(vec1, vec2)[0][0]\n",
    "\n",
    "def glove_most_similar(word, embeddings, top_n=5):\n",
    "    \"\"\"Find most similar words using GloVe embeddings.\"\"\"\n",
    "    if word not in embeddings:\n",
    "        return []\n",
    "\n",
    "    word_vec = embeddings[word].reshape(1, -1)\n",
    "\n",
    "    # Calculate similarities with all words\n",
    "    similarities = []\n",
    "    for w, vec in embeddings.items():\n",
    "        if w != word:\n",
    "            sim = cosine_similarity(word_vec, vec.reshape(1, -1))[0][0]\n",
    "            similarities.append((w, sim))\n",
    "\n",
    "    # Sort and return top N\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "def glove_analogy(a, b, c, embeddings, top_n=3):\n",
    "    \"\"\"Solve analogy: a is to b as c is to ?\"\"\"\n",
    "    if a not in embeddings or b not in embeddings or c not in embeddings:\n",
    "        return []\n",
    "\n",
    "    # target = b - a + c\n",
    "    target = embeddings[b] - embeddings[a] + embeddings[c]\n",
    "    target = target.reshape(1, -1)\n",
    "\n",
    "    # Find most similar\n",
    "    exclude = {a, b, c}\n",
    "    similarities = []\n",
    "    for w, vec in embeddings.items():\n",
    "        if w not in exclude:\n",
    "            sim = cosine_similarity(target, vec.reshape(1, -1))[0][0]\n",
    "            similarities.append((w, sim))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pre-trained GloVe\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Pre-trained GloVe Embeddings\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Similar words\n",
    "print(\"\\nğŸ“Œ Most Similar Words:\")\n",
    "for word in ['king', 'computer', 'happy']:\n",
    "    similar = glove_most_similar(word, glove_embeddings, top_n=5)\n",
    "    print(f\"\\n'{word}':\")\n",
    "    for w, score in similar:\n",
    "        print(f\"   {w}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Famous analogies with GloVe\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Word Analogies with Pre-trained GloVe\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analogies = [\n",
    "    ('man', 'king', 'woman'),       # man:king :: woman:?\n",
    "    ('paris', 'france', 'tokyo'),   # paris:france :: tokyo:?\n",
    "    ('slow', 'slower', 'fast'),     # slow:slower :: fast:?\n",
    "    ('good', 'best', 'bad'),        # good:best :: bad:?\n",
    "]\n",
    "\n",
    "print(\"\\nAnalogy: A is to B as C is to ?\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for a, b, c in analogies:\n",
    "    result = glove_analogy(a, b, c, glove_embeddings, top_n=3)\n",
    "    print(f\"\\n{a}:{b} :: {c}:?\")\n",
    "    for word, score in result:\n",
    "        print(f\"   {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Comparing Word2Vec and GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our trained Word2Vec to dictionary for comparison\n",
    "w2v_embeddings = {word: w2v_skipgram.wv[word] for word in w2v_skipgram.wv.key_to_index}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Comparison: Word2Vec vs GloVe\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare similarities for common words\n",
    "test_pairs = [\n",
    "    ('king', 'queen'),\n",
    "    ('man', 'woman'),\n",
    "    ('good', 'bad'),\n",
    "    ('happy', 'sad')\n",
    "]\n",
    "\n",
    "print(\"\\nSimilarity Scores:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Word Pair':<20} {'Word2Vec':>15} {'GloVe':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for w1, w2 in test_pairs:\n",
    "    # Word2Vec similarity\n",
    "    try:\n",
    "        w2v_sim = w2v_skipgram.wv.similarity(w1, w2)\n",
    "    except:\n",
    "        w2v_sim = None\n",
    "\n",
    "    # GloVe similarity\n",
    "    glove_sim = glove_similarity(w1, w2, glove_embeddings)\n",
    "\n",
    "    w2v_str = f\"{w2v_sim:.4f}\" if w2v_sim else \"N/A\"\n",
    "    glove_str = f\"{glove_sim:.4f}\" if glove_sim else \"N/A\"\n",
    "\n",
    "    print(f\"{w1+' - '+w2:<20} {w2v_str:>15} {glove_str:>15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical comparison\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Theoretical Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Aspect          â”‚ Word2Vec               â”‚ GloVe                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Approach        â”‚ Predictive (NN)        â”‚ Count-based + Matrix   â”‚\n",
    "â”‚ Training Input  â”‚ Local context windows  â”‚ Global co-occurrence   â”‚\n",
    "â”‚ Objective       â”‚ Predict context/center â”‚ Reconstruct log-counts â”‚\n",
    "â”‚ Memory          â”‚ Low (streaming)        â”‚ High (full matrix)     â”‚\n",
    "â”‚ Speed           â”‚ Faster for large data  â”‚ Faster for small vocab â”‚\n",
    "â”‚ Rare Words      â”‚ Better (Skip-gram)     â”‚ Depends on frequency   â”‚\n",
    "â”‚ Parallelization â”‚ Limited                â”‚ Easy                   â”‚\n",
    "â”‚ Interpretabilityâ”‚ Less intuitive         â”‚ More intuitive         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "print(comparison)\n",
    "\n",
    "print(\"\\nğŸ’¡ When to use which:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Word2Vec: Large corpus, streaming data, rare words important\")\n",
    "print(\"â€¢ GloVe: Fixed corpus, interpretability needed, parallel training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Visualization: t-SNE\n",
    "\n",
    "Let's visualize word embeddings in 2D using t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select words for visualization\n",
    "\n",
    "word_groups = {\n",
    "    'royalty': ['king', 'queen', 'prince', 'princess', 'throne', 'crown', 'royal'],\n",
    "    'family': ['man', 'woman', 'boy', 'girl', 'father', 'mother', 'son', 'daughter'],\n",
    "    'animals': ['dog', 'cat', 'horse', 'bird', 'fish', 'lion', 'tiger', 'elephant'],\n",
    "    'numbers': ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight'],\n",
    "    'countries': ['america', 'china', 'japan', 'france', 'germany', 'india', 'russia', 'brazil']\n",
    "}\n",
    "\n",
    "# Collect words that exist in GloVe\n",
    "words_to_plot = []\n",
    "colors = []\n",
    "color_map = {'royalty': 'red', 'family': 'blue', 'animals': 'green',\n",
    "             'numbers': 'orange', 'countries': 'purple'}\n",
    "\n",
    "for group, words in word_groups.items():\n",
    "    for word in words:\n",
    "        if word in glove_embeddings:\n",
    "            words_to_plot.append(word)\n",
    "            colors.append(color_map[group])\n",
    "\n",
    "print(f\"Words to visualize: {len(words_to_plot)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix and apply t-SNE\n",
    "\n",
    "embedding_matrix = np.array([glove_embeddings[word] for word in words_to_plot])\n",
    "\n",
    "print(\"Applying t-SNE (this may take a moment)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(words_to_plot)-1))\n",
    "embeddings_2d = tsne.fit_transform(embedding_matrix)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot t-SNE visualization\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot points\n",
    "for i, word in enumerate(words_to_plot):\n",
    "    x, y = embeddings_2d[i]\n",
    "    plt.scatter(x, y, c=colors[i], s=100, alpha=0.7)\n",
    "    plt.annotate(word, (x, y), fontsize=10, ha='center', va='bottom',\n",
    "                 xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w',\n",
    "                               markerfacecolor=color, markersize=10, label=group)\n",
    "                   for group, color in color_map.items()]\n",
    "plt.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "\n",
    "plt.title('t-SNE Visualization of GloVe Word Embeddings', fontsize=14)\n",
    "plt.xlabel('Dimension 1', fontsize=12)\n",
    "plt.ylabel('Dimension 2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('tsne_glove.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Notice how semantically similar words cluster together!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Practical Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application 1: Sentence Similarity\n",
    "\n",
    "def sentence_embedding(sentence, embeddings, method='average'):\n",
    "    \"\"\"Convert sentence to embedding by averaging word vectors.\"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    word_vecs = [embeddings[w] for w in words if w in embeddings]\n",
    "\n",
    "    if not word_vecs:\n",
    "        return None\n",
    "\n",
    "    if method == 'average':\n",
    "        return np.mean(word_vecs, axis=0)\n",
    "    elif method == 'sum':\n",
    "        return np.sum(word_vecs, axis=0)\n",
    "\n",
    "def sentence_similarity(sent1, sent2, embeddings):\n",
    "    \"\"\"Calculate similarity between two sentences.\"\"\"\n",
    "    emb1 = sentence_embedding(sent1, embeddings)\n",
    "    emb2 = sentence_embedding(sent2, embeddings)\n",
    "\n",
    "    if emb1 is None or emb2 is None:\n",
    "        return None\n",
    "\n",
    "    return cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
    "\n",
    "# Test sentence similarity\n",
    "print(\"=\"*60)\n",
    "print(\"Application: Sentence Similarity\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sentence_pairs = [\n",
    "    (\"The cat sat on the mat\", \"A kitten was sitting on the rug\"),\n",
    "    (\"I love programming in Python\", \"Python is my favorite language\"),\n",
    "    (\"The weather is nice today\", \"Machine learning is interesting\"),\n",
    "]\n",
    "\n",
    "print(\"\\nSentence Pair Similarity:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for sent1, sent2 in sentence_pairs:\n",
    "    sim = sentence_similarity(sent1, sent2, glove_embeddings)\n",
    "    print(f\"\\nSentence 1: {sent1}\")\n",
    "    print(f\"Sentence 2: {sent2}\")\n",
    "    print(f\"Similarity: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application 2: Simple Text Classification with Embeddings\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample dataset\n",
    "texts = [\n",
    "    # Positive\n",
    "    \"I love this product it is amazing\",\n",
    "    \"This is wonderful and fantastic\",\n",
    "    \"Great experience highly recommend\",\n",
    "    \"Excellent quality very happy\",\n",
    "    \"Best purchase ever so satisfied\",\n",
    "    \"Really good impressed with quality\",\n",
    "    # Negative\n",
    "    \"This is terrible waste of money\",\n",
    "    \"Very disappointed not worth it\",\n",
    "    \"Poor quality would not recommend\",\n",
    "    \"Horrible experience very upset\",\n",
    "    \"Bad product total disappointment\",\n",
    "    \"Awful never buying again\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# Convert texts to embeddings\n",
    "X = []\n",
    "valid_labels = []\n",
    "for text, label in zip(texts, labels):\n",
    "    emb = sentence_embedding(text, glove_embeddings)\n",
    "    if emb is not None:\n",
    "        X.append(emb)\n",
    "        valid_labels.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(valid_labels)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Application: Sentiment Classification\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset: {len(X)} samples\")\n",
    "print(f\"Embedding shape: {X.shape}\")\n",
    "\n",
    "# Train classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.2%}\")\n",
    "\n",
    "# Test on new sentences\n",
    "print(\"\\nPredictions on new sentences:\")\n",
    "print(\"-\" * 50)\n",
    "new_texts = [\n",
    "    \"This is absolutely wonderful\",\n",
    "    \"I hate this terrible product\",\n",
    "    \"Okay not bad could be better\"\n",
    "]\n",
    "\n",
    "for text in new_texts:\n",
    "    emb = sentence_embedding(text, glove_embeddings)\n",
    "    if emb is not None:\n",
    "        pred = clf.predict(emb.reshape(1, -1))[0]\n",
    "        prob = clf.predict_proba(emb.reshape(1, -1))[0]\n",
    "        sentiment = \"Positive ğŸ˜Š\" if pred == 1 else \"Negative ğŸ˜\"\n",
    "        print(f\"\\n'{text}'\")\n",
    "        print(f\"   Prediction: {sentiment} (confidence: {max(prob):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Word Embeddings** represent words as dense vectors capturing semantic meaning\n",
    "\n",
    "2. **Word2Vec** (Google, 2013):\n",
    "   - Uses neural network to predict context\n",
    "   - Two variants: Skip-gram and CBOW\n",
    "   - Works with local context windows\n",
    "\n",
    "3. **GloVe** (Stanford, 2014):\n",
    "   - Uses global co-occurrence statistics\n",
    "   - Optimizes to reconstruct log co-occurrence counts\n",
    "   - More interpretable training objective\n",
    "\n",
    "4. **Both capture**:\n",
    "   - Semantic similarity (king â‰ˆ queen)\n",
    "   - Analogical relationships (king - man + woman â‰ˆ queen)\n",
    "\n",
    "5. **Applications**:\n",
    "   - Text classification\n",
    "   - Sentiment analysis\n",
    "   - Named entity recognition\n",
    "   - Machine translation\n",
    "   - Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š SUMMARY: Word2Vec vs GloVe\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        WORD EMBEDDINGS                            â”‚\n",
    "â”‚                                                                   â”‚\n",
    "â”‚  Traditional (One-Hot, BoW)      â†’    Dense Embeddings            â”‚\n",
    "â”‚  - Sparse, high-dimensional           - Dense, low-dimensional    â”‚\n",
    "â”‚  - No semantic meaning                - Capture semantics         â”‚\n",
    "â”‚  - Words independent                  - Similar words â†’ similar   â”‚\n",
    "â”‚                                         vectors                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                   â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n",
    "â”‚  â”‚    WORD2VEC      â”‚          â”‚      GLOVE       â”‚               â”‚\n",
    "â”‚  â”‚   (Google 2013)  â”‚          â”‚  (Stanford 2014) â”‚               â”‚\n",
    "â”‚  â”‚                  â”‚          â”‚                  â”‚               â”‚\n",
    "â”‚  â”‚  \"Predictive\"    â”‚          â”‚  \"Count-based\"   â”‚               â”‚\n",
    "â”‚  â”‚                  â”‚          â”‚                  â”‚               â”‚\n",
    "â”‚  â”‚  Skip-gram:      â”‚          â”‚  Co-occurrence   â”‚               â”‚\n",
    "â”‚  â”‚  center â†’ contextâ”‚          â”‚  matrix + Matrix â”‚               â”‚\n",
    "â”‚  â”‚                  â”‚          â”‚  factorization   â”‚               â”‚\n",
    "â”‚  â”‚  CBOW:           â”‚          â”‚                  â”‚               â”‚\n",
    "â”‚  â”‚  context â†’ centerâ”‚          â”‚                  â”‚               â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚\n",
    "â”‚                                                                   â”‚\n",
    "â”‚  Both achieve: king - man + woman â‰ˆ queen                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "print(summary)\n",
    "\n",
    "print(\"\\nğŸ“ Next Steps:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Try training on your own corpus\")\n",
    "print(\"2. Experiment with different hyperparameters\")\n",
    "print(\"3. Explore FastText (handles out-of-vocabulary words)\")\n",
    "print(\"4. Learn about contextualized embeddings (BERT, ELMo)\")\n",
    "print(\"\\nâœ… Tutorial Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "1. **Train Word2Vec on a different corpus** (e.g., Wikipedia articles)\n",
    "2. **Compare Skip-gram vs CBOW** on the same data\n",
    "3. **Create a simple search engine** using sentence embeddings\n",
    "4. **Visualize word clusters** for your domain-specific vocabulary\n",
    "5. **Build a text classifier** for your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise starter code\n",
    "\n",
    "# Exercise 1: Train on custom data\n",
    "# your_corpus = [\n",
    "#     \"Your sentence here\",\n",
    "#     \"Another sentence\",\n",
    "#     ...\n",
    "# ]\n",
    "# your_sentences = [sent.lower().split() for sent in your_corpus]\n",
    "# your_model = Word2Vec(your_sentences, vector_size=100, window=5, min_count=1, epochs=50)\n",
    "\n",
    "print(\"ğŸ’¡ Uncomment and modify the code above to train on your own data!\")"
   ]
  }
 ]
}
